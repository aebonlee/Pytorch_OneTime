{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8XHHT99-Slm"
   },
   "source": [
    "### Chapter 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT ëª¨ë¸**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaV6bKLx-Sls"
   },
   "source": [
    "> ## í•™ìŠµ ëª©í‘œ\n",
    "- BERT ëª¨ë¸ì˜ ì–‘ë°©í–¥ ë¬¸ë§¥ ì¸ì½”ë”© ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ê³ , ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ í™œìš©í•œ BERT ëª¨ë¸ì˜ í”„ë¦¬íŠ¸ë ˆì´ë‹ ê³¼ì •ì„ ì´í•´í•˜ê³ , íŠ¹ì • íƒœìŠ¤í¬ì— ë§ì¶° íš¨ê³¼ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤.\n",
    "- BERT ëª¨ë¸ í•™ìŠµ ì‹œ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ê³¼ ì •ê·œí™” ë°©ë²•ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.\n",
    "- BERT ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •í™•íˆ í‰ê°€í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ ì§€í‘œë¥¼ ì„ íƒí•˜ì—¬ í™œìš©í•˜ë©°, ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 BERT ëª¨ë¸ ê°œë…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BERT (Bidirectional Encoder Representations from Transformers) ëª¨ë¸**\n",
    "\n",
    "- **2018ë…„ Googleì´ ë°œí‘œí•œ í˜ì‹ ì ì¸ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸**ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ í•µì‹¬ íŠ¹ì§•ê³¼ ì‘ë™ ë°©ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "### 15.1.1 BERTì˜ í•µì‹¬ íŠ¹ì§•\n",
    "\n",
    "1.  **ì–‘ë°©í–¥ì„± (Bidirectional Nature**) : BERTëŠ” ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì–‘ë°©í–¥ìœ¼ë¡œ(ì¦‰, ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ë¬¸ë§¥ì„ ëª¨ë‘) ì´í•´í•©ë‹ˆë‹¤. \n",
    "\n",
    "    ì´ì „ ëª¨ë¸(ì˜ˆ: Word2Vec, GloVe, GPT)ì€ ë‹¨ë°©í–¥ ë˜ëŠ” ì œí•œëœ ë¬¸ë§¥ ì •ë³´ë§Œì„ ì‚¬ìš©í–ˆì§€ë§Œ, BERTëŠ” ë¬¸ë§¥ì˜ ì „í›„ ê´€ê³„ë¥¼ ì™„ì „íˆ ê³ ë ¤í•©ë‹ˆë‹¤.\n",
    "\n",
    "2.  **Transformer ê¸°ë°˜**: BERTëŠ” Transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ì¥ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ë¬¸ë§¥ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "    íŠ¹íˆ, Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•´ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. \n",
    "\n",
    "    Transformerì˜ Encoder ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ë©°, ì´ëŠ” ì£¼ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ ì´í•´í•˜ëŠ” ë° ì´ˆì ì´ ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 15.1.2 BERTì˜ í•™ìŠµ ê³¼ì •\n",
    "\n",
    "BERTì˜ í•™ìŠµì€ ë‘ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:\n",
    "\n",
    "1.  **ì‚¬ì „ í›ˆë ¨ (Pretraining)**:\n",
    "    \n",
    "    -   ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ì ì¸ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "    -   ë‘ ê°€ì§€ ì£¼ìš” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:  \n",
    "        a) **Masked Language Model (MLM)**: ì…ë ¥ ë¬¸ì¥ì˜ ì¼ë¶€ ë‹¨ì–´ë¥¼ [MASK]ë¡œ ê°€ë¦¬ê³  í•´ë‹¹ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ\n",
    "        \n",
    "        b) **Next Sentence Prediction (NSP)**: ë‘ ë¬¸ì¥ì´ ì—°ì†ë˜ëŠ”ì§€ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ì„ í†µí•´ ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€\n",
    "    \n",
    "2.  **ë¯¸ì„¸ ì¡°ì • (Fine-tuning)**:\n",
    "    \n",
    "    -   ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŠ¹ì • NLP ì‘ì—…(ì˜ˆ: ê°ì„± ë¶„ì„, ì§ˆë¬¸ ë‹µë³€)ì— ë§ê²Œ ì¶”ê°€ë¡œ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.\n",
    "    \n",
    "\n",
    "### 15.1.3 BERTì˜ ì…ë ¥ êµ¬ì¡°\n",
    "\n",
    "BERTëŠ” íŠ¹ë³„í•œ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ì„ êµ¬ì¡°í™”í•©ë‹ˆë‹¤:\n",
    "\n",
    "-   **\\[CLS\\]**: ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ë©°, ì „ì²´ ë¬¸ì¥ì˜ íŠ¹ì„±ì„ ë‹´ëŠ” í† í°\n",
    "-   **\\[SEP\\]**: ë¬¸ì¥ ê°„ì˜ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, \"Hello world\"ë¼ëŠ” ë¬¸ì¥ì˜ BERT ì…ë ¥ì€ \"\\[CLS\\] Hello world \\[SEP\\]\"ì™€ ê°™ì€ í˜•íƒœê°€ ë©ë‹ˆë‹¤. \n",
    "\n",
    "ì´ëŸ¬í•œ êµ¬ì¡°ì™€ í•™ìŠµ ë°©ì‹ ë•ë¶„ì— BERTëŠ” ë‹¤ì–‘í•œ NLP ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, í˜„ëŒ€ ìì—°ì–´ ì²˜ë¦¬ì˜ ê¸°ë°˜ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/15.1_BERT.png\" width=\"800\"/>\n",
    "<figcaption>ê·¸ë¦¼ 15.1 BERT ëª¨ë¸ êµ¬ì¡° (ì¶œì²˜ : https://velog.io/@tm011899/BERT-ì–¸ì–´ëª¨ë¸)</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.4 BERTë¥¼ í™œìš©í•œ ê°„ë‹¨í•œ ì˜ˆì œ (PyTorch)\n",
    "Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ BERT ëª¨ë¸ì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ì˜ˆì œì—ì„œëŠ” BERTë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¶ˆëŸ¬ì™€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "-   ê°„ë‹¨í•œ `ê°ì • ë¶„ì„` ë°ì´í„°ì…‹ì„ ë‘ ë¬¸ì¥ (ê¸ì •/ë¶€ì •)ë¡œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "-   `TextDataset` í´ë˜ìŠ¤ë¥¼ í†µí•´ BERT í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•˜ê³  í•„ìš”í•œ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ëª¨ë¸ ì •ì˜\n",
    "\n",
    "-   `BertModel` ìƒìœ„ì— ë“œë¡­ì•„ì›ƒ ë° ì„ í˜• ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê°ì • ë¶„ì„ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "-   ì‚¬ì „ í›ˆë ¨ëœ `bert-base-multilingual-cased` ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### í›ˆë ¨ ë° í‰ê°€\n",
    "\n",
    "-   `train_epoch`ì™€ `eval_model` í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.\n",
    "-   ê° ì—í¬í¬ë§ˆë‹¤ í›ˆë ¨ ì†ì‹¤ê³¼ ì •í™•ë„, ê²€ì¦ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.49709808826446533 accuracy 0.0\n",
      "Validation loss 0.7336928844451904 accuracy 0.0\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.38030924399693805 accuracy 0.6666666666666666\n",
      "Validation loss 0.9019508361816406 accuracy 0.0\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.26949959993362427 accuracy 0.6666666666666666\n",
      "Validation loss 1.0163507461547852 accuracy 0.0\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.4198499818642934 accuracy 0.6666666666666666\n",
      "Validation loss 1.0626834630966187 accuracy 0.0\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.18223141630490622 accuracy 1.0\n",
      "Validation loss 0.7242190837860107 accuracy 0.0\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.1421044667561849 accuracy 1.0\n",
      "Validation loss 0.38604605197906494 accuracy 1.0\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.1138739064335823 accuracy 1.0\n",
      "Validation loss 0.24797004461288452 accuracy 1.0\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.08482220023870468 accuracy 1.0\n",
      "Validation loss 0.177778959274292 accuracy 1.0\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.05591553946336111 accuracy 1.0\n",
      "Validation loss 0.14751079678535461 accuracy 1.0\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.05022215470671654 accuracy 1.0\n",
      "Validation loss 0.1252937614917755 accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ê°„ë‹¨í•œ ë°ì´í„°ì…‹\n",
    "data = [\n",
    "    (\"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”!\", 1),\n",
    "    (\"ë³„ë¡œ ì¬ë¯¸ì—†ì—ˆì–´ìš”.\", 0),\n",
    "    (\"ì •ë§ ìµœê³ ì˜ˆìš”!\", 1),\n",
    "    (\"ì‹œê°„ë‚­ë¹„ì˜€ì–´ìš”.\", 0),\n",
    "]\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°\n",
    "texts, labels = zip(*data)\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT Tokenizer ìƒì„±\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "MAX_LEN = 20\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "train_dataset = TextDataset(texts_train, labels_train, tokenizer, MAX_LEN)\n",
    "val_dataset = TextDataset(texts_val, labels_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# BERT ëª¨ë¸ ì •ì˜\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = SentimentClassifier(n_classes=2)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# í›ˆë ¨ í•¨ìˆ˜ ì •ì˜\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# í›ˆë ¨ ë° í‰ê°€\n",
    "EPOCHS = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device, len(train_dataset))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device, len(val_dataset))\n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1\\. **í›ˆë ¨ ì†ì‹¤(Train Loss) = 0.0502**\n",
    "\n",
    "-   í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì†ì‹¤ ê°’ì…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì˜ ë§ì¶°ì¡Œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "-   ì´ ê°’ì´ **0.0502**ë¡œ ë§¤ìš° ì‘ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì•„ì£¼ ì˜ í•™ìŠµë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    " 2\\. **í›ˆë ¨ ì •í™•ë„(Train Accuracy) = 1.0**\n",
    "\n",
    "-   í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì´ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨ì…ë‹ˆë‹¤. **1.0**ì´ë¼ëŠ” ê°’ì€ í›ˆë ¨ ë°ì´í„°ì—ì„œ ëª¨ë¸ì´ **100% ì •í™•ë„**ë¥¼ ë‹¬ì„±í–ˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "-   ì´ ê°’ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ë§¤ìš° ì˜ ë§ì¶°ì¡Œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í›ˆë ¨ ì •í™•ë„ê°€ ì§€ë‚˜ì¹˜ê²Œ ë†’ë‹¤ë©´ **ê³¼ì í•©(Overfitting)**ì˜ ê°€ëŠ¥ì„±ë„ ìˆìŠµë‹ˆë‹¤. ê³¼ì í•©ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ë„ˆë¬´ íŠ¹í™”ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì¼ë°˜í™”í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\n",
    "\n",
    " 3\\. **ê²€ì¦ ì†ì‹¤(Validation Loss) = 0.1253**\n",
    "\n",
    "-   ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì†ì‹¤ ê°’ì…ë‹ˆë‹¤. í›ˆë ¨ ì†ì‹¤ì— ë¹„í•´ ì•½ê°„ ë” ë†’ì§€ë§Œ ì—¬ì „íˆ ë‚®ì€ ê°’ìœ¼ë¡œ, ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ëª¨ë¸ì´ ì˜ ì‘ë™í•˜ê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "-   **0.1253**ì´ë¼ëŠ” ê°’ì€ í›ˆë ¨ ë°ì´í„°ì— ë¹„í•´ ì†ì‹¤ì´ ì•½ê°„ ì»¤ì¡Œì§€ë§Œ ì—¬ì „íˆ ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    " 4\\. **ê²€ì¦ ì •í™•ë„(Validation Accuracy) = 1.0**\n",
    "\n",
    "-   ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì´ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨ì…ë‹ˆë‹¤. **1.0**ì´ë¼ëŠ” ê°’ì€ ê²€ì¦ ë°ì´í„°ì—ì„œ ëª¨ë¸ì´ **100% ì •í™•ë„**ë¥¼ ë‹¬ì„±í–ˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "-   í›ˆë ¨ ì •í™•ë„ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê²€ì¦ ì •í™•ë„ë„ 100%ë¼ë©´ ëª¨ë¸ì´ ê²€ì¦ ë°ì´í„°ì—ë„ ë§¤ìš° ì˜ ë§ì¶°ì¡Œë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.5819 accuracy 0.2500\n",
      "Validation loss 0.5582 accuracy 0.0000\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.5180 accuracy 0.2500\n",
      "Validation loss 0.5483 accuracy 0.5000\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.4522 accuracy 0.7500\n",
      "Validation loss 0.5438 accuracy 0.5000\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.4289 accuracy 0.6250\n",
      "Validation loss 0.5285 accuracy 0.5000\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.3637 accuracy 1.0000\n",
      "Validation loss 0.4578 accuracy 0.5000\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.3022 accuracy 0.8750\n",
      "Validation loss 0.3710 accuracy 1.0000\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.2163 accuracy 0.8750\n",
      "Validation loss 0.3132 accuracy 1.0000\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.1394 accuracy 1.0000\n",
      "Validation loss 0.2583 accuracy 1.0000\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.1249 accuracy 1.0000\n",
      "Validation loss 0.1951 accuracy 1.0000\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.0834 accuracy 1.0000\n",
      "Validation loss 0.1242 accuracy 1.0000\n",
      "ê°ì • ë¶„ì„ì„ ìœ„í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'ê·¸ë§Œ' ì…ë ¥):\n",
      "Chatbot: ì´ ë¬¸ì¥ì€ ê¸ì •ì…ë‹ˆë‹¤.\n",
      "Chatbot: ì´ ë¬¸ì¥ì€ ì¤‘ë¦½ì…ë‹ˆë‹¤.\n",
      "Chatbot: ì´ ë¬¸ì¥ì€ ë¶€ì •ì…ë‹ˆë‹¤.\n",
      "Chatbot: ì´ ë¬¸ì¥ì€ ì¤‘ë¦½ì…ë‹ˆë‹¤.\n",
      "Chatbot: ì´ ë¬¸ì¥ì€ ê¸ì •ì…ë‹ˆë‹¤.\n",
      "í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# ê°„ë‹¨í•œ ë°ì´í„°ì…‹ (ê¸ì •, ë¶€ì •, ì¤‘ë¦½)\n",
    "data = [\n",
    "    (\"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”!\", 1),\n",
    "    (\"ë³„ë¡œ ì¬ë¯¸ì—†ì—ˆì–´ìš”.\", 0),\n",
    "    (\"ì •ë§ ìµœê³ ì˜ˆìš”!\", 1),\n",
    "    (\"ì‹œê°„ë‚­ë¹„ì˜€ì–´ìš”.\", 0),\n",
    "    (\"ê·¸ëƒ¥ ê·¸ë¬ì–´ìš”.\", 2),\n",
    "    (\"í‰ë²”í–ˆì–´ìš”.\", 2),\n",
    "    (\"ì¢‹ì•˜ì–´ìš”.\", 1),\n",
    "    (\"ë³„ë¡œì˜€ì–´ìš”.\", 0),\n",
    "    (\"ê·¸ëŸ­ì €ëŸ­ì´ì—ˆì–´ìš”.\", 2),\n",
    "    (\"ê´œì°®ì•˜ì–´ìš”.\", 2),\n",
    "]\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°\n",
    "texts, labels = zip(*data)\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT Tokenizer ìƒì„±\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "MAX_LEN = 20\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "train_dataset = TextDataset(texts_train, labels_train, tokenizer, MAX_LEN)\n",
    "val_dataset = TextDataset(texts_val, labels_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# BERT ëª¨ë¸ ì •ì˜\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = SentimentClassifier(n_classes=3)  # ê¸ì •, ë¶€ì •, ì¤‘ë¦½\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# í›ˆë ¨ í•¨ìˆ˜ ì •ì˜\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# í›ˆë ¨ ë° í‰ê°€\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device, len(train_dataset))\n",
    "    print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device, len(val_dataset))\n",
    "    print(f'Validation loss {val_loss:.4f} accuracy {val_acc:.4f}')\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), 'sentiment_model.bin')\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (weights_only=True ì‚¬ìš©)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    model.load_state_dict(torch.load('sentiment_model.bin', map_location=device, weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ê°ì • í´ë˜ìŠ¤ ì •ì˜\n",
    "sentiment_classes = ['ë¶€ì •', 'ê¸ì •', 'ì¤‘ë¦½']\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def predict_sentiment(text):\n",
    "    # ì…ë ¥ ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì§•í•˜ê³  íŒ¨ë”© ì ìš©\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, prediction = torch.max(outputs, dim=1)\n",
    "\n",
    "    return sentiment_classes[prediction.item()]\n",
    "\n",
    "# ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ì•„ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” ë£¨í”„\n",
    "print(\"ê°ì • ë¶„ì„ì„ ìœ„í•œ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'ê·¸ë§Œ' ì…ë ¥):\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() == 'ê·¸ë§Œ':\n",
    "        print(\"í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "    sentiment = predict_sentiment(user_input)\n",
    "    print(f\"Chatbot: ì´ ë¬¸ì¥ì€ {sentiment}ì…ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 ëª¨ë¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT ëª¨ë¸ì„ ì‚¬ìš©í•´ ë¬¸ì¥ì—ì„œ [MASK]ë¡œ ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. \n",
    "- BertForMaskedLM ëª¨ë¸ì€ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë¡œ, ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ë¬¸ì¥: The quick brown fox jumps over the lazy [MASK].\n",
      "ì˜ˆì¸¡ëœ ë‹¨ì–´: water\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# BERTëŠ” ë§ˆìŠ¤í¬ í† í°ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í›ˆë ¨ëœ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "# ì˜ˆì‹œ ë¬¸ì¥ì—ì„œ ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "text = \"The quick brown fox jumps over the lazy [MASK].\"\n",
    "\n",
    "# ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•©ë‹ˆë‹¤.\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "model.eval()\n",
    "\n",
    "# ë§ˆìŠ¤í¬ í† í° ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# ë§ˆìŠ¤í¬ëœ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
    "mask_token_logits = logits[0, mask_token_index]\n",
    "mask_token_probs = softmax(mask_token_logits, dim=-1)\n",
    "\n",
    "# ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "predicted_token_id = torch.argmax(mask_token_probs).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"ì…ë ¥ ë¬¸ì¥: {text}\")\n",
    "print(f\"ì˜ˆì¸¡ëœ ë‹¨ì–´: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°** : BertTokenizer.from_pretrained()ë¡œ BERT í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , BertForMaskedLM.from_pretrained()ë¡œ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸(Masked Language Model)ë¡œ í›ˆë ¨ëœ BERT ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "- **ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„** : text ë³€ìˆ˜ì— ì˜ˆì‹œ ë¬¸ì¥ì„ ì‘ì„±í•˜ê³ , ê·¸ ì¤‘ ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ë‹¨ì–´ëŠ” [MASK]ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **í† í¬ë‚˜ì´ì§•** : tokenizer(text, return_tensors=\"pt\")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³ , PyTorch í…ì„œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **ëª¨ë¸ í‰ê°€** : ëª¨ë¸ì„ eval() ëª¨ë“œë¡œ ì„¤ì •í•˜ì—¬ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•˜ë©°, ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ì¶œë ¥(logits)ì„ ì–»ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **ë§ˆìŠ¤í¬ëœ í† í° ì˜ˆì¸¡** : ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜(mask_token_index)ë¥¼ ì°¾ê³ , í•´ë‹¹ ìœ„ì¹˜ì˜ ì¶œë ¥ì—ì„œ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•˜ë©°, softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥  ê°’ì„ ê³„ì‚°í•˜ê³ , ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **ì˜ˆì¸¡ëœ ë‹¨ì–´ ì¶œë ¥** : ì˜ˆì¸¡ëœ í† í° IDë¥¼ tokenizer.decode()ë¡œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë‹¨ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 10ê°œì˜ ë‹¨ì–´ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [MASK] ìœ„ì¹˜ì— ìƒìœ„ 10ê°œ ë‹¨ì–´ ì˜ˆì¸¡\n",
    "2. matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ëœ ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì„ ë°” ì°¨íŠ¸ë¡œ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water: 0.1065\n",
      "river: 0.0641\n",
      "grass: 0.0578\n",
      "stream: 0.0385\n",
      "lake: 0.0205\n",
      "brook: 0.0203\n",
      "pond: 0.0190\n",
      "rocks: 0.0111\n",
      "wind: 0.0098\n",
      "fox: 0.0096\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ì˜ˆì‹œ ë¬¸ì¥ (ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ ì˜ˆì¸¡)\n",
    "text = \"The quick brown fox jumps over the lazy [MASK].\"\n",
    "\n",
    "# ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "model.eval()\n",
    "\n",
    "# ë§ˆìŠ¤í¬ í† í° ìœ„ì¹˜ ì°¾ê¸°\n",
    "mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# ë§ˆìŠ¤í¬ëœ í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬\n",
    "mask_token_logits = logits[0, mask_token_index]\n",
    "mask_token_probs = softmax(mask_token_logits, dim=-1)\n",
    "\n",
    "# ìƒìœ„ 10ê°œì˜ ì˜ˆì¸¡ëœ ë‹¨ì–´ì™€ ê·¸ í™•ë¥ \n",
    "top_k = 10\n",
    "top_k_indices = torch.topk(mask_token_probs, top_k).indices\n",
    "top_k_probs = mask_token_probs[top_k_indices]\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "# ì˜ˆì¸¡ëœ ë‹¨ì–´ë“¤ ì¶œë ¥\n",
    "for token, prob in zip(top_k_tokens, top_k_probs):\n",
    "    print(f\"{token}: {prob.item():.4f}\")\n",
    "\n",
    "# ì‹œê°í™”: ìƒìœ„ 10ê°œ ì˜ˆì¸¡ëœ ë‹¨ì–´ì™€ ê·¸ í™•ë¥ ì„ ë°” ì°¨íŠ¸ë¡œ ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_k_tokens, top_k_probs.numpy(), color='skyblue')\n",
    "plt.xlabel('Probability')\n",
    "plt.title('Top 10 Predicted Words for [MASK] Token')\n",
    "plt.gca().invert_yaxis()  # ìƒìœ„ í•­ëª©ì´ ìœ„ì— ì˜¤ë„ë¡ ì„¤ì •\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ìƒìœ„ 10ê°œ ì˜ˆì¸¡ ë‹¨ì–´ ì¶”ì¶œ** : torch.topk()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ 10ê°œì˜ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³ , tokenizer.convert_ids_to_tokens()ë¡œ IDë¥¼ ë‹¤ì‹œ ë‹¨ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **í™•ë¥  ì¶œë ¥** : ê° ë‹¨ì–´ì™€ ê·¸ í™•ë¥  ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **ì‹œê°í™”** : matplotlib.pyplotì˜ barh()ë¥¼ ì‚¬ìš©í•´ ìƒìœ„ 10ê°œ ë‹¨ì–´ì™€ ê·¸ í™•ë¥ ì„ ìˆ˜í‰ ë°” ì°¨íŠ¸ë¡œ ì‹œê°í™”í•˜ê³ , invert_yaxis()ë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ê°€ ìœ„ì— ì˜¤ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 ì„±ë³„ ì§ˆë¬¸ BERT ëª¨ë¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "- ì´ ì˜ˆì œì—ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì„±ë³„ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì´ ì‚¬ëŒì€ ë‚¨ì„±ì¸ê°€ìš”, ì—¬ì„±ì¸ê°€ìš”?ì™€ ê°™ì€ ë¬¸ì¥ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì´ ë°ì´í„°ë¥¼ ì§ì ‘ ì‘ì„±í•˜ê±°ë‚˜ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„°ì…‹ì„ ê°„ë‹¨íˆ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.2.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be95f41d105b453ba6bdf28ca3480b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import Dataset\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°ì…‹\n",
    "data = [\n",
    "    {\"text\": \"ì´ ì‚¬ëŒì€ ë‚¨ì„±ì¸ê°€ìš”?\", \"label\": 0},  # 0: ë‚¨ì„±\n",
    "    {\"text\": \"ì´ ì‚¬ëŒì€ ì—¬ì„±ì¸ê°€ìš”?\", \"label\": 1},  # 1: ì—¬ì„±\n",
    "    {\"text\": \"ì´ ì‚¬ëŒì€ ë‚¨ìì¸ê°€ìš”?\", \"label\": 0},  # 0: ë‚¨ì„±\n",
    "    {\"text\": \"ì´ ì‚¬ëŒì€ ì—¬ìì¸ê°€ìš”?\", \"label\": 1},  # 1: ì—¬ì„±\n",
    "]\n",
    "\n",
    "# Hugging Faceì˜ Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": [item[\"text\"] for item in data],\n",
    "    \"label\": [item[\"label\"] for item in data]\n",
    "})\n",
    "\n",
    "# BERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True)\n",
    "\n",
    "# ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ì ìš©\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# DataLoader ì¤€ë¹„ - PyTorch í…ì„œë¡œ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# DataLoader ì¤€ë¹„\n",
    "train_dataloader = DataLoader(encoded_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ëª¨ë¸ ì¤€ë¹„\n",
    "- BERT ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , ì„±ë³„ì„ ë¶„ë¥˜í•  ìˆ˜ ìˆë„ë¡ fine-tuning í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# AdamW ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. í›ˆë ¨ ë£¨í”„\n",
    "- ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ë£¨í”„ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7117\n",
      "Loss: 0.7442\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ í•¨ìˆ˜\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    for batch in dataloader:\n",
    "        # ë°°ì¹˜ê°€ ì´ì œ torch í…ì„œ í˜•íƒœì´ë¯€ë¡œ 'to(device)'ë¡œ GPUë¡œ ì´ë™\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # ì†ì‹¤ ê°’\n",
    "        loss.backward()  # ì—­ì „íŒŒ\n",
    "        optimizer.step()  # ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„ ì—…ë°ì´íŠ¸\n",
    "        \n",
    "        print(f\"Loss: {loss.item():.4f}\")  # ì†ì‹¤ ê°’ ì¶œë ¥\n",
    "\n",
    "# í›ˆë ¨ ì‹¤í–‰\n",
    "train(model, train_dataloader, optimizer, device)\n",
    "\n",
    "# í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # í‰ê°€ì‹œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ í•˜ì§€ ì•ŠìŒ\n",
    "        for batch in dataloader:\n",
    "            # ë°°ì¹˜ê°€ ì´ì œ torch í…ì„œ í˜•íƒœì´ë¯€ë¡œ 'to(device)'ë¡œ GPUë¡œ ì´ë™\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡\n",
    "\n",
    "            correct += (predictions == labels).sum().item()  # ì˜ˆì¸¡ì´ ë§ëŠ” ìˆ˜\n",
    "            total += labels.size(0)  # ì´ ìƒ˜í”Œ ìˆ˜\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "evaluate(model, train_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"Loss: {loss.item():.4f}\")ì—ì„œ .item()ì€ í…ì„œë¥¼ Python ìˆ«ì íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ë©°, :.4fëŠ” ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT ëª¨ë¸ì´ ì„±ë³„ ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•´ fine-tuningë˜ê³ , í›ˆë ¨ í›„ ì •í™•ë„ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì˜ˆì œëŠ” ê¸°ë³¸ì ì¸ êµ¬ì¡°ë¡œ, ì‹¤ì œ ì„±ë³„ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•í•  ë•Œì—ëŠ” ë” ë§ì€ ë°ì´í„°ì™€, ë” ë³µì¡í•œ ì „ì²˜ë¦¬ ë° í‰ê°€ ì ˆì°¨ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ë” ë§ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì™€ ë‹¤ì–‘í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ì½”ë“œ ì„¤ëª…\n",
    "\n",
    "#### ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "* **`load_dataset('imdb')`**: Hugging Faceì˜ `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì˜í™” ë¦¬ë·°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸ì •ì /ë¶€ì •ì  ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "* **`tokenizer`**: BERT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ BERT ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í°ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "* **`tokenize_function`**: í…ìŠ¤íŠ¸ë¥¼ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í† í°í™”í•©ë‹ˆë‹¤. `padding='max_length'`ì™€ `truncation=True`ëŠ” ëª¨ë“  ë¬¸ì¥ì„ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©í•˜ê±°ë‚˜ ì˜ë¼ë‚´ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "* **`train_dataset`ê³¼ `test_dataset`**: í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°ê° í† í°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ëª¨ë¸ ì¤€ë¹„\n",
    "\n",
    "* **`BertForSequenceClassification`**: BERT ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•´ `num_labels=2`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤ (ê¸ì •/ë¶€ì • 2ê°€ì§€ í´ë˜ìŠ¤).\n",
    "* **`AdamW`**: AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\n",
    "\n",
    "* **`train` í•¨ìˆ˜**: ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³ , ê° ë°°ì¹˜ì— ëŒ€í•´ ì†ì‹¤ì„ ê³„ì‚°í•œ í›„ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "* **`evaluate` í•¨ìˆ˜**: ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œ í›„ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### í•™ìŠµ ë° í‰ê°€ ë£¨í”„\n",
    "\n",
    "* **`num_epochs`**: ì´ í•™ìŠµ íšŸìˆ˜ì…ë‹ˆë‹¤. ê° ì—í­ë§ˆë‹¤ í•™ìŠµì„ ì§„í–‰í•˜ê³  í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 4. ì‹¤í–‰ ê²°ê³¼\n",
    "\n",
    "ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ í•™ìŠµ ì¤‘ ì†ì‹¤ ê°’ê³¼ í‰ê°€ í›„ ì •í™•ë„ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. ì´ ì˜ˆì œëŠ” ê°„ë‹¨í•œ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ì„ ìœ„í•œ BERT ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  í‰ê°€í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n",
    "\n",
    "### 5. ì°¸ê³  ì‚¬í•­\n",
    "\n",
    "* **GPU ì‚¬ìš©**: `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` ì½”ë“œì—ì„œ GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¥¼, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ CPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "* **ë°°ì¹˜ í¬ê¸°**: `train_dataloader`ì™€ `test_dataloader`ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ 16ìœ¼ë¡œ ì„¤ì •í–ˆì§€ë§Œ, í•„ìš”ì— ë”°ë¼ ì´ ê°’ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 6. ìµœì¢… ëª©í‘œ\n",
    "\n",
    "ì´ ì½”ë“œëŠ” BERT ëª¨ë¸ì„ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•˜ì—¬, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…(ì—¬ê¸°ì„œëŠ” ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„)ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤. ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹¤ì–‘í•œ NLP ì‘ì—…ì— BERTë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://velog.io/@seolini43/ì¼ìƒì—°ì• -ì£¼ì œì˜-í•œêµ­ì–´-ëŒ€í™”-BERTë¡œ-ì´ì§„-ë¶„ë¥˜-ëª¨ë¸-ë§Œë“¤ê¸°íŒŒì´ì¬Colab-ì½”ë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„ with Hugging Face BERT**\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)ëŠ” êµ¬ê¸€ì´ ê°œë°œí•œ ì‚¬ì „í›ˆë ¨(pre-training) ëª¨ë¸ì…ë‹ˆë‹¤. \n",
    "\n",
    "ìœ„í‚¤í”¼ë””ì•„ ê°™ì€ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ ë¯¸ë¦¬ í•™ìŠµì„ í•˜ë©´, ì–¸ì–´ì˜ ê¸°ë³¸ì ì¸ íŒ¨í„´ì„ ì´í•´í•œ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. \n",
    "\n",
    "ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì œì— ì ìš©í•˜ëŠ” ì „ì´í•™ìŠµ(transfer learning)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì¢€ ë” ì ì€ ë°ì´í„°ë¡œ ë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ìµœê·¼ ìì—°ì–´ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ë²•ìœ¼ë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì˜ˆì œì—ì„œëŠ” í•œê¸€ NLPì˜ Hello worldë¼ê³  í•  ìˆ˜ ìˆëŠ” ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. \n",
    "\n",
    "ê°€ì¥ ìœ ëª…í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ Hugging Faceì˜ PyTorch BERTë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ Chris McCormickì˜ ë¸”ë¡œê·¸ë¥¼ ì°¸ì¡°í•˜ì—¬ í•œê¸€ì— ë§ê²Œ ìˆ˜ì •í•˜ì˜€ìŒì„ ë¯¸ë¦¬ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.\n",
    "\n",
    "< BERT Fine-Tuning Tutorial with PyTorch ><br>\n",
    "-> https://mccormickml.com/2019/07/22/BERT-fine-tuning\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "BERTì— ëŒ€í•´ì„œ ì¢€ ë” ìì„¸í•œ ì„¤ëª…ì€ ì•„ë˜ ë¸”ë¡œê·¸ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "\n",
    "< BERT í†ºì•„ë³´ê¸° ><br>\n",
    "-> http://docs.likejazz.com/bert/\n",
    "\n",
    "< The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) ><br>\n",
    "-> http://jalammar.github.io/illustrated-bert/\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ì¤€ë¹„ ì‚¬í•­**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì„¤ì¹˜\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ë°ì´í„° ë¡œë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nsmc'...\n",
      "Updating files:   3% (461/14737)\n",
      "Updating files:   4% (590/14737)\n",
      "Updating files:   5% (737/14737)\n",
      "Updating files:   6% (885/14737)\n",
      "Updating files:   7% (1032/14737)\n",
      "Updating files:   8% (1179/14737)\n",
      "Updating files:   8% (1192/14737)\n",
      "Updating files:   9% (1327/14737)\n",
      "Updating files:  10% (1474/14737)\n",
      "Updating files:  11% (1622/14737)\n",
      "Updating files:  12% (1769/14737)\n",
      "Updating files:  13% (1916/14737)\n",
      "Updating files:  13% (1924/14737)\n",
      "Updating files:  14% (2064/14737)\n",
      "Updating files:  15% (2211/14737)\n",
      "Updating files:  16% (2358/14737)\n",
      "Updating files:  17% (2506/14737)\n",
      "Updating files:  18% (2653/14737)\n",
      "Updating files:  18% (2697/14737)\n",
      "Updating files:  19% (2801/14737)\n",
      "Updating files:  20% (2948/14737)\n",
      "Updating files:  21% (3095/14737)\n",
      "Updating files:  22% (3243/14737)\n",
      "Updating files:  22% (3339/14737)\n",
      "Updating files:  23% (3390/14737)\n",
      "Updating files:  24% (3537/14737)\n",
      "Updating files:  25% (3685/14737)\n",
      "Updating files:  26% (3832/14737)\n",
      "Updating files:  27% (3979/14737)\n",
      "Updating files:  27% (4087/14737)\n",
      "Updating files:  28% (4127/14737)\n",
      "Updating files:  29% (4274/14737)\n",
      "Updating files:  30% (4422/14737)\n",
      "Updating files:  31% (4569/14737)\n",
      "Updating files:  32% (4716/14737)\n",
      "Updating files:  32% (4813/14737)\n",
      "Updating files:  33% (4864/14737)\n",
      "Updating files:  34% (5011/14737)\n",
      "Updating files:  35% (5158/14737)\n",
      "Updating files:  36% (5306/14737)\n",
      "Updating files:  37% (5453/14737)\n",
      "Updating files:  37% (5547/14737)\n",
      "Updating files:  38% (5601/14737)\n",
      "Updating files:  39% (5748/14737)\n",
      "Updating files:  40% (5895/14737)\n",
      "Updating files:  41% (6043/14737)\n",
      "Updating files:  41% (6172/14737)\n",
      "Updating files:  42% (6190/14737)\n",
      "Updating files:  43% (6337/14737)\n",
      "Updating files:  44% (6485/14737)\n",
      "Updating files:  45% (6632/14737)\n",
      "Updating files:  46% (6780/14737)\n",
      "Updating files:  47% (6927/14737)\n",
      "Updating files:  47% (6967/14737)\n",
      "Updating files:  48% (7074/14737)\n",
      "Updating files:  49% (7222/14737)\n",
      "Updating files:  50% (7369/14737)\n",
      "Updating files:  51% (7516/14737)\n",
      "Updating files:  51% (7646/14737)\n",
      "Updating files:  52% (7664/14737)\n",
      "Updating files:  53% (7811/14737)\n",
      "Updating files:  54% (7958/14737)\n",
      "Updating files:  55% (8106/14737)\n",
      "Updating files:  56% (8253/14737)\n",
      "Updating files:  56% (8313/14737)\n",
      "Updating files:  57% (8401/14737)\n",
      "Updating files:  58% (8548/14737)\n",
      "Updating files:  59% (8695/14737)\n",
      "Updating files:  60% (8843/14737)\n",
      "Updating files:  60% (8923/14737)\n",
      "Updating files:  61% (8990/14737)\n",
      "Updating files:  62% (9137/14737)\n",
      "Updating files:  63% (9285/14737)\n",
      "Updating files:  64% (9432/14737)\n",
      "Updating files:  64% (9563/14737)\n",
      "Updating files:  65% (9580/14737)\n",
      "Updating files:  66% (9727/14737)\n",
      "Updating files:  67% (9874/14737)\n",
      "Updating files:  68% (10022/14737)\n",
      "Updating files:  68% (10161/14737)\n",
      "Updating files:  69% (10169/14737)\n",
      "Updating files:  70% (10316/14737)\n",
      "Updating files:  71% (10464/14737)\n",
      "Updating files:  72% (10611/14737)\n",
      "Updating files:  73% (10759/14737)\n",
      "Updating files:  74% (10906/14737)\n",
      "Updating files:  75% (11053/14737)\n",
      "Updating files:  76% (11201/14737)\n",
      "Updating files:  76% (11239/14737)\n",
      "Updating files:  77% (11348/14737)\n",
      "Updating files:  78% (11495/14737)\n",
      "Updating files:  79% (11643/14737)\n",
      "Updating files:  80% (11790/14737)\n",
      "Updating files:  80% (11878/14737)\n",
      "Updating files:  81% (11937/14737)\n",
      "Updating files:  82% (12085/14737)\n",
      "Updating files:  83% (12232/14737)\n",
      "Updating files:  84% (12380/14737)\n",
      "Updating files:  84% (12440/14737)\n",
      "Updating files:  85% (12527/14737)\n",
      "Updating files:  86% (12674/14737)\n",
      "Updating files:  87% (12822/14737)\n",
      "Updating files:  88% (12969/14737)\n",
      "Updating files:  88% (13033/14737)\n",
      "Updating files:  89% (13116/14737)\n",
      "Updating files:  90% (13264/14737)\n",
      "Updating files:  91% (13411/14737)\n",
      "Updating files:  92% (13559/14737)\n",
      "Updating files:  92% (13631/14737)\n",
      "Updating files:  93% (13706/14737)\n",
      "Updating files:  94% (13853/14737)\n",
      "Updating files:  95% (14001/14737)\n",
      "Updating files:  96% (14148/14737)\n",
      "Updating files:  96% (14221/14737)\n",
      "Updating files:  97% (14295/14737)\n",
      "Updating files:  98% (14443/14737)\n",
      "Updating files:  99% (14590/14737)\n",
      "Updating files: 100% (14737/14737)\n",
      "Updating files: 100% (14737/14737), done.\n"
     ]
    }
   ],
   "source": [
    "# ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "!git clone https://github.com/aebonlee/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„ ë°ì´í„°ë¥¼ Githubì—ì„œ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ nsmc ë””ë ‰í† ë¦¬ì— ìˆëŠ” ratings_train.txtì™€ ratings_test.txtë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ë ‰í† ë¦¬ './nsmc'ì˜ íŒŒì¼ ëª©ë¡:\n",
      ".git\n",
      "code\n",
      "ratings.txt\n",
      "ratings_test.txt\n",
      "ratings_train.txt\n",
      "raw\n",
      "README.md\n",
      "synopses.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import locale\n",
    "\n",
    "# í•œê¸€ ë¡œì¼€ì¼ ì„¤ì • (Linux/Windows ëª¨ë‘ í˜¸í™˜)\n",
    "locale.setlocale(locale.LC_ALL, 'ko_KR.UTF-8')\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "directory = \"./nsmc\"\n",
    "\n",
    "# íŒŒì¼ ëª©ë¡ ì¶œë ¥\n",
    "try:\n",
    "    files = os.listdir(directory)\n",
    "    print(f\"ë””ë ‰í† ë¦¬ '{directory}'ì˜ íŒŒì¼ ëª©ë¡:\")\n",
    "    for file in files:\n",
    "        print(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ë””ë ‰í† ë¦¬ '{directory}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "# ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡\n",
    "# !dir nsmc -la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "# íŒë‹¤ìŠ¤ë¡œ í›ˆë ¨ì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ ë°ì´í„° ë¡œë“œ\n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ì…‹ 150,000ê°œì™€ í…ŒìŠ¤íŠ¸ì…‹ 50,000ê°œì˜ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5403919</td>\n",
       "      <td>ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7797314</td>\n",
       "      <td>ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9443947</td>\n",
       "      <td>ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7156791</td>\n",
       "      <td>ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5912145</td>\n",
       "      <td>ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
       "1   3819312                  í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
       "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
       "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •      0\n",
       "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1\n",
       "5   5403919      ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€.      0\n",
       "6   7797314                              ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤.      0\n",
       "7   9443947  ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨...      0\n",
       "8   7156791                             ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”      1\n",
       "9   5912145      ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜?      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í›ˆë ¨ì…‹ì˜ ì•ë¶€ë¶„ ì¶œë ¥\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idëŠ” íšŒì›ì •ë³´, documentëŠ” ë¦¬ë·° ë¬¸ì¥ì…ë‹ˆë‹¤. labelì´ 0ì´ë©´ ë¶€ì •, 1ì´ë©´ ê¸ì •ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. idëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— documentì™€ labelë§Œ ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ì „ì²˜ë¦¬ - í›ˆë ¨ì…‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\n",
       "1                    í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\n",
       "2                                    ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤\n",
       "3                        êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •\n",
       "4    ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...\n",
       "5        ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€.\n",
       "6                                ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤.\n",
       "7    ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨...\n",
       "8                               ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”\n",
       "9        ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜?\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¦¬ë·° ë¬¸ì¥ ì¶”ì¶œ\n",
    "sentences = train['document']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP]',\n",
       " '[CLS] í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜ [SEP]',\n",
       " '[CLS] ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤ [SEP]',\n",
       " '[CLS] êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì • [SEP]',\n",
       " '[CLS] ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ë˜ìŠ¤íŠ¸ê°€ ë„ˆë¬´ë‚˜ë„ ì´ë»ë³´ì˜€ë‹¤ [SEP]',\n",
       " '[CLS] ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™”.ã…‹ã…‹ã…‹...ë³„ë°˜ê°œë„ ì•„ê¹Œì›€. [SEP]',\n",
       " '[CLS] ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤. [SEP]',\n",
       " '[CLS] ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€..ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨ ë‚«ê²Ÿë‹¤ ë‚©ì¹˜.ê°ê¸ˆë§Œë°˜ë³µë°˜ë³µ..ì´ë“œë¼ë§ˆëŠ” ê°€ì¡±ë„ì—†ë‹¤ ì—°ê¸°ëª»í•˜ëŠ”ì‚¬ëŒë§Œëª¨ì—¿ë„¤ [SEP]',\n",
       " '[CLS] ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™” [SEP]',\n",
       " '[CLS] ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë°? ê½¤ ë³¼ë§Œí•œë°.. í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜? [SEP]']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ëŒ€ì²´ í…ìŠ¤íŠ¸](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n",
    "\n",
    "BERTì˜ ì…ë ¥ì€ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ í˜•ì‹ì…ë‹ˆë‹¤. Classificationì„ ëœ»í•˜ëŠ” [CLS] ì‹¬ë³¼ì´ ì œì¼ ì•ì— ì‚½ì…ë©ë‹ˆë‹¤. íŒŒì¸íŠœë‹ì‹œ ì¶œë ¥ì—ì„œ ì´ ìœ„ì¹˜ì˜ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ë¥¼ í•©ë‹ˆë‹¤. [SEP]ì€ Seperationì„ ê°€ë¦¬í‚¤ëŠ”ë°, ë‘ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” ë¬¸ì¥ì´ í•˜ë‚˜ì´ë¯€ë¡œ [SEP]ë„ í•˜ë‚˜ë§Œ ë„£ìŠµë‹ˆë‹¤.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ë²¨ ì¶”ì¶œ\n",
    "labels = train['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬ [SEP]\n",
      "['[CLS]', 'ì•„', 'ë”', '##ë¹™', '.', '.', 'ì§„', '##ì§œ', 'ì§œ', '##ì¦', '##ë‚˜', '##ë„¤', '##ìš”', 'ëª©', '##ì†Œ', '##ë¦¬', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTëŠ” í˜•íƒœì†Œë¶„ì„ìœ¼ë¡œ í† í°ì„ ë¶„ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. WordPieceë¼ëŠ” í†µê³„ì ì¸ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•œ ë‹¨ì–´ë‚´ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ê¸€ìë“¤ì„ ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì–¸ì–´ì— ìƒê´€ì—†ì´ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‹ ì¡°ì–´ ê°™ì´ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ê¸°ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìœ„ì˜ ê²°ê³¼ì—ì„œ ## ê¸°í˜¸ëŠ” ì• í† í°ê³¼ ì´ì–´ì§„ë‹¤ëŠ” í‘œì‹œì…ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì—¬ëŸ¬ ì–¸ì–´ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“  'bert-base-multilingual-cased'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ í•œê¸€ë„ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519,   9074, 119005,    119,    119,   9708, 119235,\n",
       "         9715, 119230,  16439,  77884,  48549,   9284,  22333,  12692,\n",
       "          102,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "MAX_LEN = 128\n",
    "\n",
    "# í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³´í†µ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ëŠ” í† í° ìì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ë² ë”© ë ˆì´ì–´ì—ëŠ” í† í°ì„ ìˆ«ìë¡œ ëœ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. BERTì˜ í† í¬ë‚˜ì´ì €ëŠ” {ë‹¨ì–´í† í°:ì¸ë±ìŠ¤}ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ì‚¬ì „ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì°¸ì¡°í•˜ì—¬ í† í°ì„ ì¸ë±ìŠ¤ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”\n",
    "attention_masks = []\n",
    "\n",
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "# íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9711,  11489,   9364,  41850,   9004,  32537,   9491,  35506,\n",
      "         17360,  48549,    119,    119,   9477,  26444,  12692,   9665,  21789,\n",
      "         11287,   9708, 119235,   9659,  22458, 119136,  12965,  48549,    119,\n",
      "           119,   9532,  22879,   9685,  16985,  14523,  48549,    119,    119,\n",
      "          9596, 118728,    119,    119,   9178, 106065, 118916,    119,    119,\n",
      "          8903,  11664,  11513,   9960,  14423,  25503, 118671,  48549,    119,\n",
      "           119,  21890,   9546,  37819,  22879,   9356,  14867,   9715, 119230,\n",
      "        118716,  48345,    119,   9663,  23321,  10954,   9638,  35506, 106320,\n",
      "         10739,  20173,   9359,  19105,  11102,  42428,  17196,  48549,    119,\n",
      "           119,    100,    117,   9947,  12945,   9532,  25503,   8932,  14423,\n",
      "         35506, 119050,  11903,  14867,  10003,  14863,  33188,  48345,    119,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "tensor([   101,   1871, 111754, 111754, 111754, 111754,    102,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels,\n",
    "                                                                                    random_state=2018,\n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                    input_ids,\n",
    "                                                    random_state=2018,\n",
    "                                                    test_size=0.1)\n",
    "\n",
    "# ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "batch_size = 32\n",
    "\n",
    "# íŒŒì´í† ì¹˜ì˜ DataLoaderë¡œ ì…ë ¥, ë§ˆìŠ¤í¬, ë¼ë²¨ì„ ë¬¶ì–´ ë°ì´í„° ì„¤ì •\n",
    "# í•™ìŠµì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ì „ì²˜ë¦¬ - í…ŒìŠ¤íŠ¸ì…‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  êµ³ ã…‹\n",
       "1                                 GDNTOPCLASSINTHECLUB\n",
       "2               ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„\n",
       "3                     ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....\n",
       "4    3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??\n",
       "5                                   ìŒì•…ì´ ì£¼ê°€ ëœ, ìµœê³ ì˜ ìŒì•…ì˜í™”\n",
       "6                                              ì§„ì •í•œ ì“°ë ˆê¸°\n",
       "7             ë§ˆì¹˜ ë¯¸êµ­ì• ë‹ˆì—ì„œ íŠ€ì–´ë‚˜ì˜¨ë“¯í•œ ì°½ì˜ë ¥ì—†ëŠ” ë¡œë´‡ë””ìì¸ë¶€í„°ê°€,ê³ ê°œë¥¼ ì –ê²Œí•œë‹¤\n",
       "8    ê°ˆìˆ˜ë¡ ê°œíŒë˜ê°€ëŠ” ì¤‘êµ­ì˜í™” ìœ ì¹˜í•˜ê³  ë‚´ìš©ì—†ìŒ í¼ì¡ë‹¤ ëë‚¨ ë§ë„ì•ˆë˜ëŠ” ë¬´ê¸°ì— ìœ ì¹˜í•œc...\n",
       "9       ì´ë³„ì˜ ì•„í””ë’¤ì— ì°¾ì•„ì˜¤ëŠ” ìƒˆë¡œìš´ ì¸ì—°ì˜ ê¸°ì¨ But, ëª¨ë“  ì‚¬ëŒì´ ê·¸ë ‡ì§€ëŠ” ì•Šë„¤..\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¦¬ë·° ë¬¸ì¥ ì¶”ì¶œ\n",
    "sentences = test['document']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] êµ³ ã…‹ [SEP]',\n",
       " '[CLS] GDNTOPCLASSINTHECLUB [SEP]',\n",
       " '[CLS] ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„ [SEP]',\n",
       " '[CLS] ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”.... [SEP]',\n",
       " '[CLS] 3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ?? [SEP]',\n",
       " '[CLS] ìŒì•…ì´ ì£¼ê°€ ëœ, ìµœê³ ì˜ ìŒì•…ì˜í™” [SEP]',\n",
       " '[CLS] ì§„ì •í•œ ì“°ë ˆê¸° [SEP]',\n",
       " '[CLS] ë§ˆì¹˜ ë¯¸êµ­ì• ë‹ˆì—ì„œ íŠ€ì–´ë‚˜ì˜¨ë“¯í•œ ì°½ì˜ë ¥ì—†ëŠ” ë¡œë´‡ë””ìì¸ë¶€í„°ê°€,ê³ ê°œë¥¼ ì –ê²Œí•œë‹¤ [SEP]',\n",
       " '[CLS] ê°ˆìˆ˜ë¡ ê°œíŒë˜ê°€ëŠ” ì¤‘êµ­ì˜í™” ìœ ì¹˜í•˜ê³  ë‚´ìš©ì—†ìŒ í¼ì¡ë‹¤ ëë‚¨ ë§ë„ì•ˆë˜ëŠ” ë¬´ê¸°ì— ìœ ì¹˜í•œcgë‚¨ë¬´ ì•„ ê·¸ë¦½ë‹¤ ë™ì‚¬ì„œë…ê°™ì€ ì˜í™”ê°€ ì´ê±´ 3ë¥˜ì•„ë¥˜ì‘ì´ë‹¤ [SEP]',\n",
       " '[CLS] ì´ë³„ì˜ ì•„í””ë’¤ì— ì°¾ì•„ì˜¤ëŠ” ìƒˆë¡œìš´ ì¸ì—°ì˜ ê¸°ì¨ But, ëª¨ë“  ì‚¬ëŒì´ ê·¸ë ‡ì§€ëŠ” ì•Šë„¤.. [SEP]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ë²¨ ì¶”ì¶œ\n",
    "labels = test['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] êµ³ ã…‹ [SEP]\n",
      "['[CLS]', 'êµ³', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "MAX_LEN = 128\n",
    "\n",
    "# í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”\n",
    "attention_masks = []\n",
    "\n",
    "# ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "# íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "batch_size = 32\n",
    "\n",
    "# íŒŒì´í† ì¹˜ì˜ DataLoaderë¡œ ì…ë ¥, ë§ˆìŠ¤í¬, ë¼ë²¨ì„ ë¬¶ì–´ ë°ì´í„° ì„¤ì •\n",
    "# í•™ìŠµì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ëª¨ë¸ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)  # GPU ë””ë°”ì´ìŠ¤ ì´ë¦„ \n",
    "    print(f'Found GPU: {device_name}')\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ëª¨ë¸ ìƒì„±\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ëŒ€ì²´ í…ìŠ¤íŠ¸](http://www.mccormickml.com/assets/BERT/padding_and_mask.png)\n",
    "\n",
    "ì‚¬ì „í›ˆë ¨ëœ BERTëŠ” ë‹¤ì–‘í•œ ë¬¸ì œë¡œ ì „ì´í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ í•œ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜í™”ë¦¬ë·° ë¬¸ì¥ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ë©´, ê¸ì •/ë¶€ì •ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ [CLS] ìœ„ì¹˜ì¸ ì²« ë²ˆì§¸ í† í°ì— ìƒˆë¡œìš´ ë ˆì´ì–´ë¥¼ ë¶™ì—¬ì„œ íŒŒì¸íŠœë‹ì„ í•©ë‹ˆë‹¤. Huggning FaceëŠ” BertForSequenceClassification() í•¨ìˆ˜ë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # í•™ìŠµë¥ \n",
    "                  eps = 1e-8 # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ epsilon ê°’\n",
    "                )\n",
    "\n",
    "# ì—í­ìˆ˜\n",
    "epochs = 4\n",
    "\n",
    "# ì´ í›ˆë ¨ ìŠ¤í… : ë°°ì¹˜ë°˜ë³µ íšŸìˆ˜ * ì—í­\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# ì²˜ìŒì— í•™ìŠµë¥ ì„ ì¡°ê¸ˆì”© ë³€í™”ì‹œí‚¤ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ëª¨ë¸ í•™ìŠµ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜\n",
    "def flat_accuracy(preds, labels):\n",
    "\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ í‘œì‹œ í•¨ìˆ˜\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # ë°˜ì˜¬ë¦¼\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # hh:mm:ssìœ¼ë¡œ í˜•íƒœ ë³€ê²½\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:08:08.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 0:16:10.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 0:24:39.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 0:32:41.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 1:02:12.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 1:32:09.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 2:03:41.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 2:34:19.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 2:48:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:05:22\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:33:58.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 1:06:44.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 1:38:41.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 1:56:58.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 2:04:58.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 2:12:56.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 2:20:54.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 2:30:05.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 2:33:35\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:07:59.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 0:15:57.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 0:23:56.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 0:32:06.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 0:40:07.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 0:48:16.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 0:56:15.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 1:04:14.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epcoh took: 1:07:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:07:58.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 0:15:58.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 0:23:57.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 0:41:06.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 0:49:36.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 0:58:07.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 1:06:37.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 1:15:06.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 1:18:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:01:39\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ì¬í˜„ì„ ìœ„í•´ ëœë¤ì‹œë“œ ê³ ì •\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "model.zero_grad()\n",
    "\n",
    "# ì—í­ë§Œí¼ ë°˜ë³µ\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # ì‹œì‘ ì‹œê°„ ì„¤ì •\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ë¡œìŠ¤ ì´ˆê¸°í™”\n",
    "    total_loss = 0\n",
    "\n",
    "    # í›ˆë ¨ëª¨ë“œë¡œ ë³€ê²½\n",
    "    model.train()\n",
    "\n",
    "    # ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ê°€ì ¸ì˜´\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # ê²½ê³¼ ì •ë³´ í‘œì‹œ\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # ë°°ì¹˜ë¥¼ GPUì— ë„£ìŒ\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward ìˆ˜í–‰\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        # ë¡œìŠ¤ êµ¬í•¨\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # ì´ ë¡œìŠ¤ ê³„ì‚°\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward ìˆ˜í–‰ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "        loss.backward()\n",
    "\n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "        optimizer.step()\n",
    "\n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ í•™ìŠµë¥  ê°ì†Œ\n",
    "        scheduler.step()\n",
    "\n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        model.zero_grad()\n",
    "\n",
    "    # í‰ê·  ë¡œìŠ¤ ê³„ì‚°\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #ì‹œì‘ ì‹œê°„ ì„¤ì •\n",
    "    t0 = time.time()\n",
    "\n",
    "    # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½\n",
    "    model.eval()\n",
    "\n",
    "    # ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ê°€ì ¸ì˜´\n",
    "    for batch in validation_dataloader:\n",
    "        # ë°°ì¹˜ë¥¼ GPUì— ë„£ìŒ\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨\n",
    "        with torch.no_grad():\n",
    "            # Forward ìˆ˜í–‰\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        # ì¶œë ¥ ë¡œì§“ êµ¬í•¨\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPUë¡œ ë°ì´í„° ì´ë™\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # ì¶œë ¥ ë¡œì§“ê³¼ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—í­ë§ˆë‹¤ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ì„ ë°˜ë³µí•˜ì—¬ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,563.    Elapsed: 0:00:21.\n",
      "  Batch   200  of  1,563.    Elapsed: 0:00:42.\n",
      "  Batch   300  of  1,563.    Elapsed: 0:01:03.\n",
      "  Batch   400  of  1,563.    Elapsed: 0:01:25.\n",
      "  Batch   500  of  1,563.    Elapsed: 0:01:46.\n",
      "  Batch   600  of  1,563.    Elapsed: 0:02:07.\n",
      "  Batch   700  of  1,563.    Elapsed: 0:02:28.\n",
      "  Batch   800  of  1,563.    Elapsed: 0:02:49.\n",
      "  Batch   900  of  1,563.    Elapsed: 0:03:10.\n",
      "  Batch 1,000  of  1,563.    Elapsed: 0:03:32.\n",
      "  Batch 1,100  of  1,563.    Elapsed: 0:03:53.\n",
      "  Batch 1,200  of  1,563.    Elapsed: 0:04:14.\n",
      "  Batch 1,300  of  1,563.    Elapsed: 0:04:35.\n",
      "  Batch 1,400  of  1,563.    Elapsed: 0:04:57.\n",
      "  Batch 1,500  of  1,563.    Elapsed: 0:05:18.\n",
      "\n",
      "Accuracy: 0.87\n",
      "Test took: 0:05:31\n"
     ]
    }
   ],
   "source": [
    "#ì‹œì‘ ì‹œê°„ ì„¤ì •\n",
    "t0 = time.time()\n",
    "\n",
    "# í‰ê°€ëª¨ë“œë¡œ ë³€ê²½\n",
    "model.eval()\n",
    "\n",
    "# ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ê°€ì ¸ì˜´\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # ê²½ê³¼ ì •ë³´ í‘œì‹œ\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # ë°°ì¹˜ë¥¼ GPUì— ë„£ìŒ\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨\n",
    "    with torch.no_grad():\n",
    "        # Forward ìˆ˜í–‰\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # ì¶œë ¥ ë¡œì§“ êµ¬í•¨\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPUë¡œ ë°ì´í„° ì´ë™\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # ì¶œë ¥ ë¡œì§“ê³¼ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸ì…‹ì˜ ì •í™•ë„ê°€ 87%ì…ë‹ˆë‹¤. <BERT í†ºì•„ë³´ê¸°> ë¸”ë¡œê·¸ì—ì„œëŠ” ê°™ì€ ë°ì´í„°ë¡œ 88.7%ë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ê±°ê¸°ì„œëŠ” í•œê¸€ ì½”í¼ìŠ¤ë¡œ ì‚¬ì „í›ˆë ¨ì„ í•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë°˜ë©´ì— ìš°ë¦¬ëŠ” BERTì˜ ê¸°ë³¸ ëª¨ë¸ì¸ bert-base-multilingual-casedë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ë” ì„±ëŠ¥ì´ ë‚®ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ìƒˆë¡œìš´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ ë°ì´í„° ë³€í™˜\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERTì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # ì…ë ¥ í† í°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # í† í°ì„ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "    # ë¬¸ì¥ì„ MAX_LEN ê¸¸ì´ì— ë§ê²Œ ìë¥´ê³ , ëª¨ìë€ ë¶€ë¶„ì„ íŒ¨ë”© 0ìœ¼ë¡œ ì±„ì›€\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì´ˆê¸°í™”\n",
    "    attention_masks = []\n",
    "\n",
    "    # ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ íŒ¨ë”©ì´ ì•„ë‹ˆë©´ 1, íŒ¨ë”©ì´ë©´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "    # íŒ¨ë”© ë¶€ë¶„ì€ BERT ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ ì†ë„ í–¥ìƒ\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # ë°ì´í„°ë¥¼ íŒŒì´í† ì¹˜ì˜ í…ì„œë¡œ ë³€í™˜\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ í…ŒìŠ¤íŠ¸\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½\n",
    "    model.eval()\n",
    "\n",
    "    # ë¬¸ì¥ì„ ì…ë ¥ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # ë°ì´í„°ë¥¼ GPUì— ë„£ìŒ\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "\n",
    "    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨\n",
    "    with torch.no_grad():\n",
    "        # Forward ìˆ˜í–‰\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # ì¶œë ¥ ë¡œì§“ êµ¬í•¨\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPUë¡œ ë°ì´í„° ì´ë™\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.2769246  1.905877 ]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['ì—°ê¸°ëŠ” ë³„ë¡œì§€ë§Œ ì¬ë¯¸ í•˜ë‚˜ëŠ” ëë‚´ì¤Œ!'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3848722 -3.0497596]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['ì£¼ì—°ë°°ìš°ê°€ ì•„ê¹ë‹¤. ì´ì²´ì  ë‚œêµ­...'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµí•œ ëª¨ë¸ì„ ê°€ì§€ê³  ì‹¤ì œ ë¬¸ì¥ì„ ë„£ì–´ë´¤ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¶œë ¥ ë¡œì§“ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ê°€ ì ìš©ë˜ì§€ ì•Šì€ ìƒíƒœì…ë‹ˆë‹¤.\n",
    "\n",
    " argmaxë¡œ ë” ë†’ì€ ê°’ì˜ ìœ„ì¹˜ë¥¼ ë¼ë²¨ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. 0ì€ ë¶€ì •, 1ì€ ê¸ì •ì…ë‹ˆë‹¤. \n",
    " \n",
    " ìœ„ì™€ ê°™ì´ ìƒˆë¡œìš´ ë¬¸ì¥ì—ë„ ì˜ ë¶„ë¥˜ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< ì±—ë´‡ ê°œë°œì ëª¨ì„ ><br>\n",
    "- í˜ì´ìŠ¤ë¶ ê·¸ë£¹ì— ê°€ì…í•˜ì‹œë©´ ì±—ë´‡ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ì‰½ê²Œ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- https://www.facebook.com/groups/ChatbotDevKR/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
