{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8XHHT99-Slm"
   },
   "source": [
    "### Chapter 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaV6bKLx-Sls"
   },
   "source": [
    "> ## 학습 목표\n",
    "- BERT 모델의 양방향 문맥 인코딩 메커니즘을 이해하고, 다양한 자연어 처리 작업에 효과적으로 적용할 수 있다.\n",
    "- 대규모 데이터셋을 활용한 BERT 모델의 프리트레이닝 과정을 이해하고, 특정 태스크에 맞춰 효과적으로 파인튜닝하는 전략을 수립할 수 있다.\n",
    "- BERT 모델 학습 시 다양한 최적화 기법과 정규화 방법을 적용하여 모델의 성능을 향상시키고 과적합을 방지할 수 있다.\n",
    "- BERT 모델의 성능을 정확히 평가하고 분석할 수 있는 적절한 지표를 선택하여 활용하며, 모델을 지속적으로 개선할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 BERT 모델 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BERT (Bidirectional Encoder Representations from Transformers) 모델**\n",
    "\n",
    "- **2018년 Google이 발표한 혁신적인 자연어 처리(NLP) 모델**입니다. 이 모델의 핵심 특징과 작동 방식을 다음과 같이 설명할 수 있습니다:\n",
    "\n",
    "### 15.1.1 BERT의 핵심 특징\n",
    "\n",
    "1.  **양방향성 (Bidirectional Nature**) : BERT는 입력 문장의 모든 단어를 양방향으로(즉, 왼쪽과 오른쪽 문맥을 모두) 이해합니다. \n",
    "\n",
    "    이전 모델(예: Word2Vec, GloVe, GPT)은 단방향 또는 제한된 문맥 정보만을 사용했지만, BERT는 문맥의 전후 관계를 완전히 고려합니다.\n",
    "\n",
    "2.  **Transformer 기반**: BERT는 Transformer 아키텍처를 사용하여 긴 문장에서도 효과적으로 문맥을 파악할 수 있습니다. \n",
    "\n",
    "    특히, Self-Attention 메커니즘을 활용해 문장에서 단어들 간의 관계를 효율적으로 학습합니다. \n",
    "\n",
    "    Transformer의 Encoder 부분만 사용하며, 이는 주로 입력 데이터를 잘 이해하는 데 초점이 맞춰져 있습니다.\n",
    "\n",
    "### 15.1.2 BERT의 학습 과정\n",
    "\n",
    "BERT의 학습은 두 단계로 이루어집니다:\n",
    "\n",
    "1.  **사전 훈련 (Pretraining)**:\n",
    "    \n",
    "    -   대규모 텍스트 데이터를 사용하여 일반적인 언어 이해 능력을 학습합니다.\n",
    "    -   두 가지 주요 작업을 수행합니다:  \n",
    "        a) **Masked Language Model (MLM)**: 입력 문장의 일부 단어를 [MASK]로 가리고 해당 단어를 예측하도록 학습\n",
    "        \n",
    "        b) **Next Sentence Prediction (NSP)**: 두 문장이 연속되는지 예측하는 작업을 통해 문장 간 관계를 이해하는 데 도움\n",
    "    \n",
    "2.  **미세 조정 (Fine-tuning)**:\n",
    "    \n",
    "    -   사전 훈련된 모델을 특정 NLP 작업(예: 감성 분석, 질문 답변)에 맞게 추가로 훈련시킵니다.\n",
    "    \n",
    "\n",
    "### 15.1.3 BERT의 입력 구조\n",
    "\n",
    "BERT는 특별한 토큰을 사용하여 입력을 구조화합니다:\n",
    "\n",
    "-   **\\[CLS\\]**: 문장의 시작을 나타내며, 전체 문장의 특성을 담는 토큰\n",
    "-   **\\[SEP\\]**: 문장 간의 구분을 나타내는 토큰\n",
    "\n",
    "예를 들어, \"Hello world\"라는 문장의 BERT 입력은 \"\\[CLS\\] Hello world \\[SEP\\]\"와 같은 형태가 됩니다. \n",
    "\n",
    "이러한 구조와 학습 방식 덕분에 BERT는 다양한 NLP 작업에서 뛰어난 성능을 보여주며, 현대 자연어 처리의 기반이 되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/15.1_BERT.png\" width=\"800\"/>\n",
    "<figcaption>그림 15.1 BERT 모델 구조 (출처 : https://velog.io/@tm011899/BERT-언어모델)</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.4 BERT를 활용한 간단한 예제 (PyTorch)\n",
    "Hugging Face의 transformers 라이브러리와 PyTorch를 사용하여 BERT 모델을 이용한 텍스트 분류 작업을 수행하는 간단한 예제입니다. \n",
    "\n",
    "이 예제에서는 BERT를 사전 훈련된 모델로 불러와 텍스트를 분류하는 작업을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 준비\n",
    "\n",
    "-   간단한 `감정 분석` 데이터셋을 두 문장 (긍정/부정)로 구성했습니다.\n",
    "-   `TextDataset` 클래스를 통해 BERT 토크나이저를 사용하여 입력 문장을 토큰화하고 필요한 입력 형식으로 변환합니다.\n",
    "\n",
    "#### 모델 정의\n",
    "\n",
    "-   `BertModel` 상위에 드롭아웃 및 선형 레이어를 추가하여 감정 분석 모델을 구현합니다.\n",
    "-   사전 훈련된 `bert-base-multilingual-cased` 모델을 사용합니다.\n",
    "\n",
    "#### 훈련 및 평가\n",
    "\n",
    "-   `train_epoch`와 `eval_model` 함수를 통해 모델을 훈련하고 평가합니다.\n",
    "-   각 에포크마다 훈련 손실과 정확도, 검증 손실과 정확도를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.49709808826446533 accuracy 0.0\n",
      "Validation loss 0.7336928844451904 accuracy 0.0\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.38030924399693805 accuracy 0.6666666666666666\n",
      "Validation loss 0.9019508361816406 accuracy 0.0\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.26949959993362427 accuracy 0.6666666666666666\n",
      "Validation loss 1.0163507461547852 accuracy 0.0\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.4198499818642934 accuracy 0.6666666666666666\n",
      "Validation loss 1.0626834630966187 accuracy 0.0\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.18223141630490622 accuracy 1.0\n",
      "Validation loss 0.7242190837860107 accuracy 0.0\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.1421044667561849 accuracy 1.0\n",
      "Validation loss 0.38604605197906494 accuracy 1.0\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.1138739064335823 accuracy 1.0\n",
      "Validation loss 0.24797004461288452 accuracy 1.0\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.08482220023870468 accuracy 1.0\n",
      "Validation loss 0.177778959274292 accuracy 1.0\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.05591553946336111 accuracy 1.0\n",
      "Validation loss 0.14751079678535461 accuracy 1.0\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.05022215470671654 accuracy 1.0\n",
      "Validation loss 0.1252937614917755 accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 간단한 데이터셋\n",
    "data = [\n",
    "    (\"이 영화 정말 재미있어요!\", 1),\n",
    "    (\"별로 재미없었어요.\", 0),\n",
    "    (\"정말 최고예요!\", 1),\n",
    "    (\"시간낭비였어요.\", 0),\n",
    "]\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 데이터셋 나누기\n",
    "texts, labels = zip(*data)\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT Tokenizer 생성\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 최대 길이 설정\n",
    "MAX_LEN = 20\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataset = TextDataset(texts_train, labels_train, tokenizer, MAX_LEN)\n",
    "val_dataset = TextDataset(texts_val, labels_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# BERT 모델 정의\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = SentimentClassifier(n_classes=2)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 옵티마이저와 손실 함수 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 훈련 함수 정의\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# 평가 함수 정의\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# 훈련 및 평가\n",
    "EPOCHS = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device, len(train_dataset))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device, len(val_dataset))\n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1\\. **훈련 손실(Train Loss) = 0.0502**\n",
    "\n",
    "-   훈련 데이터에 대해 모델이 예측한 값과 실제 값 간의 차이를 나타내는 손실 값입니다. 낮을수록 모델이 훈련 데이터에 잘 맞춰졌다는 의미입니다.\n",
    "-   이 값이 **0.0502**로 매우 작다는 것은 모델이 훈련 데이터에서 아주 잘 학습되었음을 나타냅니다.\n",
    "\n",
    " 2\\. **훈련 정확도(Train Accuracy) = 1.0**\n",
    "\n",
    "-   훈련 데이터에 대해 모델이 정확하게 예측한 비율입니다. **1.0**이라는 값은 훈련 데이터에서 모델이 **100% 정확도**를 달성했다는 의미입니다.\n",
    "-   이 값은 모델이 훈련 데이터에 매우 잘 맞춰졌다는 것을 보여줍니다. 그러나 훈련 정확도가 지나치게 높다면 **과적합(Overfitting)**의 가능성도 있습니다. 과적합은 모델이 훈련 데이터에 너무 특화되어 새로운 데이터에 대해 일반화하지 못하는 문제입니다.\n",
    "\n",
    " 3\\. **검증 손실(Validation Loss) = 0.1253**\n",
    "\n",
    "-   검증 데이터에 대해 모델이 예측한 값과 실제 값 간의 차이를 나타내는 손실 값입니다. 훈련 손실에 비해 약간 더 높지만 여전히 낮은 값으로, 검증 데이터에 대해서도 모델이 잘 작동하고 있음을 나타냅니다.\n",
    "-   **0.1253**이라는 값은 훈련 데이터에 비해 손실이 약간 커졌지만 여전히 좋은 성능을 의미합니다.\n",
    "\n",
    " 4\\. **검증 정확도(Validation Accuracy) = 1.0**\n",
    "\n",
    "-   검증 데이터에 대해 모델이 정확하게 예측한 비율입니다. **1.0**이라는 값은 검증 데이터에서 모델이 **100% 정확도**를 달성했다는 의미입니다.\n",
    "-   훈련 정확도와 마찬가지로 검증 정확도도 100%라면 모델이 검증 데이터에도 매우 잘 맞춰졌다는 것을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.5819 accuracy 0.2500\n",
      "Validation loss 0.5582 accuracy 0.0000\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.5180 accuracy 0.2500\n",
      "Validation loss 0.5483 accuracy 0.5000\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.4522 accuracy 0.7500\n",
      "Validation loss 0.5438 accuracy 0.5000\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.4289 accuracy 0.6250\n",
      "Validation loss 0.5285 accuracy 0.5000\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.3637 accuracy 1.0000\n",
      "Validation loss 0.4578 accuracy 0.5000\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.3022 accuracy 0.8750\n",
      "Validation loss 0.3710 accuracy 1.0000\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.2163 accuracy 0.8750\n",
      "Validation loss 0.3132 accuracy 1.0000\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.1394 accuracy 1.0000\n",
      "Validation loss 0.2583 accuracy 1.0000\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.1249 accuracy 1.0000\n",
      "Validation loss 0.1951 accuracy 1.0000\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.0834 accuracy 1.0000\n",
      "Validation loss 0.1242 accuracy 1.0000\n",
      "감정 분석을 위한 문장을 입력하세요 (종료하려면 '그만' 입력):\n",
      "Chatbot: 이 문장은 긍정입니다.\n",
      "Chatbot: 이 문장은 중립입니다.\n",
      "Chatbot: 이 문장은 부정입니다.\n",
      "Chatbot: 이 문장은 중립입니다.\n",
      "Chatbot: 이 문장은 긍정입니다.\n",
      "프로그램을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# 간단한 데이터셋 (긍정, 부정, 중립)\n",
    "data = [\n",
    "    (\"이 영화 정말 재미있어요!\", 1),\n",
    "    (\"별로 재미없었어요.\", 0),\n",
    "    (\"정말 최고예요!\", 1),\n",
    "    (\"시간낭비였어요.\", 0),\n",
    "    (\"그냥 그랬어요.\", 2),\n",
    "    (\"평범했어요.\", 2),\n",
    "    (\"좋았어요.\", 1),\n",
    "    (\"별로였어요.\", 0),\n",
    "    (\"그럭저럭이었어요.\", 2),\n",
    "    (\"괜찮았어요.\", 2),\n",
    "]\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 데이터셋 나누기\n",
    "texts, labels = zip(*data)\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT Tokenizer 생성\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 최대 길이 설정\n",
    "MAX_LEN = 20\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataset = TextDataset(texts_train, labels_train, tokenizer, MAX_LEN)\n",
    "val_dataset = TextDataset(texts_val, labels_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# BERT 모델 정의\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = SentimentClassifier(n_classes=3)  # 긍정, 부정, 중립\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 훈련 함수 정의\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# 평가 함수 정의\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# 훈련 및 평가\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device, len(train_dataset))\n",
    "    print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device, len(val_dataset))\n",
    "    print(f'Validation loss {val_loss:.4f} accuracy {val_acc:.4f}')\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'sentiment_model.bin')\n",
    "\n",
    "# 모델 로드 (weights_only=True 사용)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    model.load_state_dict(torch.load('sentiment_model.bin', map_location=device, weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 감정 클래스 정의\n",
    "sentiment_classes = ['부정', '긍정', '중립']\n",
    "\n",
    "# 사용자 입력을 받아 감정을 예측하는 함수 정의\n",
    "def predict_sentiment(text):\n",
    "    # 입력 문장을 토크나이징하고 패딩 적용\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 모델을 사용하여 예측\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, prediction = torch.max(outputs, dim=1)\n",
    "\n",
    "    return sentiment_classes[prediction.item()]\n",
    "\n",
    "# 사용자로부터 입력을 받아 감정을 예측하는 루프\n",
    "print(\"감정 분석을 위한 문장을 입력하세요 (종료하려면 '그만' 입력):\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() == '그만':\n",
    "        print(\"프로그램을 종료합니다.\")\n",
    "        break\n",
    "    sentiment = predict_sentiment(user_input)\n",
    "    print(f\"Chatbot: 이 문장은 {sentiment}입니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 필수 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\.conda\\envs\\aebon\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT 모델을 사용해 문장에서 [MASK]로 마스크된 단어를 예측합니다. \n",
    "- BertForMaskedLM 모델은 마스크 언어 모델로, 주어진 문맥을 바탕으로 마스크된 단어를 예측하는 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: The quick brown fox jumps over the lazy [MASK].\n",
      "예측된 단어: water\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# BERT 모델과 토크나이저 불러오기\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# BERT는 마스크 토큰을 예측할 수 있도록 훈련된 모델입니다.\n",
    "# 예시 문장에서 마스크된 단어를 예측해보겠습니다.\n",
    "text = \"The quick brown fox jumps over the lazy [MASK].\"\n",
    "\n",
    "# 입력 텍스트를 토크나이징합니다.\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델을 평가 모드로 설정합니다.\n",
    "model.eval()\n",
    "\n",
    "# 마스크 토큰 위치를 찾습니다.\n",
    "mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# 모델을 통해 예측합니다.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# 마스크된 토큰에 대한 확률 분포를 얻습니다.\n",
    "mask_token_logits = logits[0, mask_token_index]\n",
    "mask_token_probs = softmax(mask_token_logits, dim=-1)\n",
    "\n",
    "# 가장 확률이 높은 단어를 예측합니다.\n",
    "predicted_token_id = torch.argmax(mask_token_probs).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"입력 문장: {text}\")\n",
    "print(f\"예측된 단어: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **모델과 토크나이저 불러오기** : BertTokenizer.from_pretrained()로 BERT 토크나이저를 불러오고, BertForMaskedLM.from_pretrained()로 마스크 언어 모델(Masked Language Model)로 훈련된 BERT 모델을 불러옵니다.\n",
    "\n",
    "- **입력 텍스트 준비** : text 변수에 예시 문장을 작성하고, 그 중 예측하고 싶은 단어는 [MASK]로 대체합니다.\n",
    "\n",
    "- **토크나이징** : tokenizer(text, return_tensors=\"pt\")를 사용하여 입력 텍스트를 토큰화하고, PyTorch 텐서 형식으로 반환합니다.\n",
    "\n",
    "- **모델 평가** : 모델을 eval() 모드로 설정하여 평가 모드로 전환하며, 모델에 입력을 전달하여 출력(logits)을 얻습니다.\n",
    "\n",
    "- **마스크된 토큰 예측** : 마스크된 위치(mask_token_index)를 찾고, 해당 위치의 출력에서 확률 분포를 계산하며, softmax를 사용하여 확률 값을 계산하고, 가장 확률이 높은 토큰을 예측합니다.\n",
    "\n",
    "- **예측된 단어 출력** : 예측된 토큰 ID를 tokenizer.decode()로 사람이 읽을 수 있는 단어로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 10개의 단어 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [MASK] 위치에 상위 10개 단어 예측\n",
    "2. matplotlib 라이브러리를 사용하여 예측된 단어들의 확률을 바 차트로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water: 0.1065\n",
      "river: 0.0641\n",
      "grass: 0.0578\n",
      "stream: 0.0385\n",
      "lake: 0.0205\n",
      "brook: 0.0203\n",
      "pond: 0.0190\n",
      "rocks: 0.0111\n",
      "wind: 0.0098\n",
      "fox: 0.0096\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# BERT 모델과 토크나이저 불러오기\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# 예시 문장 (마스크된 단어 예측)\n",
    "text = \"The quick brown fox jumps over the lazy [MASK].\"\n",
    "\n",
    "# 입력 텍스트를 토크나이징\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 마스크 토큰 위치 찾기\n",
    "mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# 모델 예측\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# 마스크된 토큰에 대한 확률 분포\n",
    "mask_token_logits = logits[0, mask_token_index]\n",
    "mask_token_probs = softmax(mask_token_logits, dim=-1)\n",
    "\n",
    "# 상위 10개의 예측된 단어와 그 확률\n",
    "top_k = 10\n",
    "top_k_indices = torch.topk(mask_token_probs, top_k).indices\n",
    "top_k_probs = mask_token_probs[top_k_indices]\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "# 예측된 단어들 출력\n",
    "for token, prob in zip(top_k_tokens, top_k_probs):\n",
    "    print(f\"{token}: {prob.item():.4f}\")\n",
    "\n",
    "# 시각화: 상위 10개 예측된 단어와 그 확률을 바 차트로 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_k_tokens, top_k_probs.numpy(), color='skyblue')\n",
    "plt.xlabel('Probability')\n",
    "plt.title('Top 10 Predicted Words for [MASK] Token')\n",
    "plt.gca().invert_yaxis()  # 상위 항목이 위에 오도록 설정\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **상위 10개 예측 단어 추출** : torch.topk()를 사용하여 가장 확률이 높은 10개의 단어를 추출하고, tokenizer.convert_ids_to_tokens()로 ID를 다시 단어로 변환합니다.\n",
    "\n",
    "- **확률 출력** : 각 단어와 그 확률 값을 출력합니다.\n",
    "\n",
    "- **시각화** : matplotlib.pyplot의 barh()를 사용해 상위 10개 단어와 그 확률을 수평 바 차트로 시각화하고, invert_yaxis()를 사용하여 확률이 높은 단어가 위에 오도록 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 성별 질문 BERT 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터셋 준비\n",
    "\n",
    "- 이 예제에서는 간단한 예시 데이터셋을 사용합니다. 성별을 예측하는 데 사용할 텍스트 데이터는 이 사람은 남성인가요, 여성인가요?와 같은 문장이 될 수 있습니다. \n",
    "\n",
    "이 데이터를 직접 작성하거나 datasets 라이브러리에서 가져올 수도 있지만, 여기서는 예시 데이터셋을 간단히 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.2.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be95f41d105b453ba6bdf28ca3480b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import Dataset\n",
    "\n",
    "# 예시 데이터셋\n",
    "data = [\n",
    "    {\"text\": \"이 사람은 남성인가요?\", \"label\": 0},  # 0: 남성\n",
    "    {\"text\": \"이 사람은 여성인가요?\", \"label\": 1},  # 1: 여성\n",
    "    {\"text\": \"이 사람은 남자인가요?\", \"label\": 0},  # 0: 남성\n",
    "    {\"text\": \"이 사람은 여자인가요?\", \"label\": 1},  # 1: 여성\n",
    "]\n",
    "\n",
    "# Hugging Face의 Dataset 객체로 변환\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": [item[\"text\"] for item in data],\n",
    "    \"label\": [item[\"label\"] for item in data]\n",
    "})\n",
    "\n",
    "# BERT 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 텍스트를 토큰화하는 함수\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True)\n",
    "\n",
    "# 데이터셋에 전처리 적용\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# DataLoader 준비 - PyTorch 텐서로 반환하도록 설정\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# DataLoader 준비\n",
    "train_dataloader = DataLoader(encoded_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 모델 준비\n",
    "- BERT 모델을 불러오고, 성별을 분류할 수 있도록 fine-tuning 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 모델 불러오기\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# AdamW 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 모델을 GPU로 이동 (가능한 경우)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 훈련 루프\n",
    "- 모델을 훈련시킬 루프를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7117\n",
      "Loss: 0.7442\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 훈련 함수\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    for batch in dataloader:\n",
    "        # 배치가 이제 torch 텐서 형태이므로 'to(device)'로 GPU로 이동\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # 옵티마이저 초기화\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # 손실 값\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 옵티마이저 단계 업데이트\n",
    "        \n",
    "        print(f\"Loss: {loss.item():.4f}\")  # 손실 값 출력\n",
    "\n",
    "# 훈련 실행\n",
    "train(model, train_dataloader, optimizer, device)\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 평가시 그래디언트 계산을 하지 않음\n",
    "        for batch in dataloader:\n",
    "            # 배치가 이제 torch 텐서 형태이므로 'to(device)'로 GPU로 이동\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)  # 가장 높은 확률의 클래스를 예측\n",
    "\n",
    "            correct += (predictions == labels).sum().item()  # 예측이 맞는 수\n",
    "            total += labels.size(0)  # 총 샘플 수\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 평가 실행\n",
    "evaluate(model, train_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"Loss: {loss.item():.4f}\")에서 .item()은 텐서를 Python 숫자 타입으로 변환하며, :.4f는 소수점 4자리까지 출력하라는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 모델이 성별 분류 작업을 위해 fine-tuning되고, 훈련 후 정확도가 출력됩니다.\n",
    "\n",
    "이 예제는 기본적인 구조로, 실제 성별 분류 모델을 구축할 때에는 더 많은 데이터와, 더 복잡한 전처리 및 평가 절차가 필요할 수 있습니다. \n",
    "\n",
    "예를 들어, 더 많은 텍스트 데이터와 다양한 문장을 사용하여 모델을 훈련하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 코드 설명\n",
    "\n",
    "#### 데이터셋 준비\n",
    "\n",
    "* **`load_dataset('imdb')`**: Hugging Face의 `datasets` 라이브러리를 사용하여 IMDB 영화 리뷰 데이터셋을 불러옵니다. 이 데이터셋은 영화 리뷰를 바탕으로 긍정적/부정적 감정을 분류하는 작업을 수행하는데 사용됩니다.\n",
    "* **`tokenizer`**: BERT 모델에서 사용하는 토크나이저를 로드합니다. 이 토크나이저는 텍스트를 BERT 모델이 처리할 수 있는 토큰으로 변환합니다.\n",
    "\n",
    "#### 데이터 전처리\n",
    "\n",
    "* **`tokenize_function`**: 텍스트를 BERT 입력 형식에 맞게 토큰화합니다. `padding='max_length'`와 `truncation=True`는 모든 문장을 최대 길이에 맞춰 패딩하거나 잘라내도록 설정합니다.\n",
    "* **`train_dataset`과 `test_dataset`**: 학습 데이터와 테스트 데이터를 각각 토큰화합니다.\n",
    "\n",
    "#### 모델 준비\n",
    "\n",
    "* **`BertForSequenceClassification`**: BERT 모델을 로드하고, 분류 작업을 위해 `num_labels=2`로 설정합니다 (긍정/부정 2가지 클래스).\n",
    "* **`AdamW`**: AdamW 옵티마이저를 사용하여 모델을 최적화합니다.\n",
    "\n",
    "#### 학습 및 평가 함수\n",
    "\n",
    "* **`train` 함수**: 모델을 학습 모드로 설정하고, 각 배치에 대해 손실을 계산한 후 역전파를 수행하여 모델의 가중치를 업데이트합니다.\n",
    "* **`evaluate` 함수**: 모델을 평가 모드로 설정하고, 테스트 데이터셋에 대해 예측을 수행한 후 정확도를 계산합니다.\n",
    "\n",
    "#### 학습 및 평가 루프\n",
    "\n",
    "* **`num_epochs`**: 총 학습 횟수입니다. 각 에폭마다 학습을 진행하고 평가를 수행합니다.\n",
    "\n",
    "### 4. 실행 결과\n",
    "\n",
    "이 코드를 실행하면 학습 중 손실 값과 평가 후 정확도가 출력됩니다. 이 예제는 간단한 영화 리뷰 감정 분석을 위한 BERT 모델을 학습시키고 평가하는 코드입니다.\n",
    "\n",
    "### 5. 참고 사항\n",
    "\n",
    "* **GPU 사용**: `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` 코드에서 GPU가 사용 가능하면 GPU를, 그렇지 않으면 CPU를 사용하도록 설정합니다.\n",
    "* **배치 크기**: `train_dataloader`와 `test_dataloader`의 배치 크기를 16으로 설정했지만, 필요에 따라 이 값을 조정할 수 있습니다.\n",
    "\n",
    "### 6. 최종 목표\n",
    "\n",
    "이 코드는 BERT 모델을 파이토치로 구현하여, 텍스트 분류 작업(여기서는 영화 리뷰 감정 분석)을 수행하는 예제입니다. 이와 같은 방식으로 다양한 NLP 작업에 BERT를 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://velog.io/@seolini43/일상연애-주제의-한국어-대화-BERT로-이진-분류-모델-만들기파이썬Colab-코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **네이버 영화리뷰 감정분석 with Hugging Face BERT**\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 구글이 개발한 사전훈련(pre-training) 모델입니다. \n",
    "\n",
    "위키피디아 같은 텍스트 코퍼스를 사용해서 미리 학습을 하면, 언어의 기본적인 패턴을 이해한 모델이 만들어집니다. \n",
    "\n",
    "이를 기반으로 새로운 문제에 적용하는 전이학습(transfer learning)을 수행합니다. 좀 더 적은 데이터로 보다 빠르게 학습이 가능하다는 장점이 있습니다. 그래서 최근 자연어처리의 핵심 기법으로 떠오르고 있습니다.\n",
    "\n",
    "이 예제에서는 한글 NLP의 Hello world라고 할 수 있는 네이버 영화리뷰 감정분석을 구현해보겠습니다. \n",
    "\n",
    "가장 유명한 모델 중 하나인 Hugging Face의 PyTorch BERT를 사용하였습니다. 아래의 Chris McCormick의 블로그를 참조하여 한글에 맞게 수정하였음을 미리 알려드립니다.\n",
    "\n",
    "< BERT Fine-Tuning Tutorial with PyTorch ><br>\n",
    "-> https://mccormickml.com/2019/07/22/BERT-fine-tuning\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "BERT에 대해서 좀 더 자세한 설명은 아래 블로그를 참조하시기 바랍니다.\n",
    "\n",
    "< BERT 톺아보기 ><br>\n",
    "-> http://docs.likejazz.com/bert/\n",
    "\n",
    "< The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) ><br>\n",
    "-> http://jalammar.github.io/illustrated-bert/\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **준비 사항**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face의 트랜스포머 모델을 설치\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **데이터 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nsmc'...\n",
      "Updating files:   3% (461/14737)\n",
      "Updating files:   4% (590/14737)\n",
      "Updating files:   5% (737/14737)\n",
      "Updating files:   6% (885/14737)\n",
      "Updating files:   7% (1032/14737)\n",
      "Updating files:   8% (1179/14737)\n",
      "Updating files:   8% (1192/14737)\n",
      "Updating files:   9% (1327/14737)\n",
      "Updating files:  10% (1474/14737)\n",
      "Updating files:  11% (1622/14737)\n",
      "Updating files:  12% (1769/14737)\n",
      "Updating files:  13% (1916/14737)\n",
      "Updating files:  13% (1924/14737)\n",
      "Updating files:  14% (2064/14737)\n",
      "Updating files:  15% (2211/14737)\n",
      "Updating files:  16% (2358/14737)\n",
      "Updating files:  17% (2506/14737)\n",
      "Updating files:  18% (2653/14737)\n",
      "Updating files:  18% (2697/14737)\n",
      "Updating files:  19% (2801/14737)\n",
      "Updating files:  20% (2948/14737)\n",
      "Updating files:  21% (3095/14737)\n",
      "Updating files:  22% (3243/14737)\n",
      "Updating files:  22% (3339/14737)\n",
      "Updating files:  23% (3390/14737)\n",
      "Updating files:  24% (3537/14737)\n",
      "Updating files:  25% (3685/14737)\n",
      "Updating files:  26% (3832/14737)\n",
      "Updating files:  27% (3979/14737)\n",
      "Updating files:  27% (4087/14737)\n",
      "Updating files:  28% (4127/14737)\n",
      "Updating files:  29% (4274/14737)\n",
      "Updating files:  30% (4422/14737)\n",
      "Updating files:  31% (4569/14737)\n",
      "Updating files:  32% (4716/14737)\n",
      "Updating files:  32% (4813/14737)\n",
      "Updating files:  33% (4864/14737)\n",
      "Updating files:  34% (5011/14737)\n",
      "Updating files:  35% (5158/14737)\n",
      "Updating files:  36% (5306/14737)\n",
      "Updating files:  37% (5453/14737)\n",
      "Updating files:  37% (5547/14737)\n",
      "Updating files:  38% (5601/14737)\n",
      "Updating files:  39% (5748/14737)\n",
      "Updating files:  40% (5895/14737)\n",
      "Updating files:  41% (6043/14737)\n",
      "Updating files:  41% (6172/14737)\n",
      "Updating files:  42% (6190/14737)\n",
      "Updating files:  43% (6337/14737)\n",
      "Updating files:  44% (6485/14737)\n",
      "Updating files:  45% (6632/14737)\n",
      "Updating files:  46% (6780/14737)\n",
      "Updating files:  47% (6927/14737)\n",
      "Updating files:  47% (6967/14737)\n",
      "Updating files:  48% (7074/14737)\n",
      "Updating files:  49% (7222/14737)\n",
      "Updating files:  50% (7369/14737)\n",
      "Updating files:  51% (7516/14737)\n",
      "Updating files:  51% (7646/14737)\n",
      "Updating files:  52% (7664/14737)\n",
      "Updating files:  53% (7811/14737)\n",
      "Updating files:  54% (7958/14737)\n",
      "Updating files:  55% (8106/14737)\n",
      "Updating files:  56% (8253/14737)\n",
      "Updating files:  56% (8313/14737)\n",
      "Updating files:  57% (8401/14737)\n",
      "Updating files:  58% (8548/14737)\n",
      "Updating files:  59% (8695/14737)\n",
      "Updating files:  60% (8843/14737)\n",
      "Updating files:  60% (8923/14737)\n",
      "Updating files:  61% (8990/14737)\n",
      "Updating files:  62% (9137/14737)\n",
      "Updating files:  63% (9285/14737)\n",
      "Updating files:  64% (9432/14737)\n",
      "Updating files:  64% (9563/14737)\n",
      "Updating files:  65% (9580/14737)\n",
      "Updating files:  66% (9727/14737)\n",
      "Updating files:  67% (9874/14737)\n",
      "Updating files:  68% (10022/14737)\n",
      "Updating files:  68% (10161/14737)\n",
      "Updating files:  69% (10169/14737)\n",
      "Updating files:  70% (10316/14737)\n",
      "Updating files:  71% (10464/14737)\n",
      "Updating files:  72% (10611/14737)\n",
      "Updating files:  73% (10759/14737)\n",
      "Updating files:  74% (10906/14737)\n",
      "Updating files:  75% (11053/14737)\n",
      "Updating files:  76% (11201/14737)\n",
      "Updating files:  76% (11239/14737)\n",
      "Updating files:  77% (11348/14737)\n",
      "Updating files:  78% (11495/14737)\n",
      "Updating files:  79% (11643/14737)\n",
      "Updating files:  80% (11790/14737)\n",
      "Updating files:  80% (11878/14737)\n",
      "Updating files:  81% (11937/14737)\n",
      "Updating files:  82% (12085/14737)\n",
      "Updating files:  83% (12232/14737)\n",
      "Updating files:  84% (12380/14737)\n",
      "Updating files:  84% (12440/14737)\n",
      "Updating files:  85% (12527/14737)\n",
      "Updating files:  86% (12674/14737)\n",
      "Updating files:  87% (12822/14737)\n",
      "Updating files:  88% (12969/14737)\n",
      "Updating files:  88% (13033/14737)\n",
      "Updating files:  89% (13116/14737)\n",
      "Updating files:  90% (13264/14737)\n",
      "Updating files:  91% (13411/14737)\n",
      "Updating files:  92% (13559/14737)\n",
      "Updating files:  92% (13631/14737)\n",
      "Updating files:  93% (13706/14737)\n",
      "Updating files:  94% (13853/14737)\n",
      "Updating files:  95% (14001/14737)\n",
      "Updating files:  96% (14148/14737)\n",
      "Updating files:  96% (14221/14737)\n",
      "Updating files:  97% (14295/14737)\n",
      "Updating files:  98% (14443/14737)\n",
      "Updating files:  99% (14590/14737)\n",
      "Updating files: 100% (14737/14737)\n",
      "Updating files: 100% (14737/14737), done.\n"
     ]
    }
   ],
   "source": [
    "# 네이버 영화리뷰 감정분석 데이터 다운로드\n",
    "!git clone https://github.com/aebonlee/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네이버 영화리뷰 감정분석 데이터를 Github에서 다운로드 합니다. 아래와 같이 nsmc 디렉토리에 있는 ratings_train.txt와 ratings_test.txt를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 './nsmc'의 파일 목록:\n",
      ".git\n",
      "code\n",
      "ratings.txt\n",
      "ratings_test.txt\n",
      "ratings_train.txt\n",
      "raw\n",
      "README.md\n",
      "synopses.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import locale\n",
    "\n",
    "# 한글 로케일 설정 (Linux/Windows 모두 호환)\n",
    "locale.setlocale(locale.LC_ALL, 'ko_KR.UTF-8')\n",
    "\n",
    "# 디렉토리 경로 설정\n",
    "directory = \"./nsmc\"\n",
    "\n",
    "# 파일 목록 출력\n",
    "try:\n",
    "    files = os.listdir(directory)\n",
    "    print(f\"디렉토리 '{directory}'의 파일 목록:\")\n",
    "    for file in files:\n",
    "        print(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"디렉토리 '{directory}'를 찾을 수 없습니다.\")\n",
    "    \n",
    "# 디렉토리의 파일 목록\n",
    "# !dir nsmc -la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
    "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련셋 150,000개와 테스트셋 50,000개의 데이터가 존재합니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5403919</td>\n",
       "      <td>막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7797314</td>\n",
       "      <td>원작의 긴장감을 제대로 살려내지못했다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9443947</td>\n",
       "      <td>별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7156791</td>\n",
       "      <td>액션이 없는데도 재미 있는 몇안되는 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5912145</td>\n",
       "      <td>왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "5   5403919      막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.      0\n",
       "6   7797314                              원작의 긴장감을 제대로 살려내지못했다.      0\n",
       "7   9443947  별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...      0\n",
       "8   7156791                             액션이 없는데도 재미 있는 몇안되는 영화      1\n",
       "9   5912145      왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련셋의 앞부분 출력\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id는 회원정보, document는 리뷰 문장입니다. label이 0이면 부정, 1이면 긍정으로 분류됩니다. id는 사용하지 않기 때문에 document와 label만 추출하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **전처리 - 훈련셋**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  아 더빙.. 진짜 짜증나네요 목소리\n",
       "1                    흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
       "2                                    너무재밓었다그래서보는것을추천한다\n",
       "3                        교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
       "4    사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...\n",
       "5        막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\n",
       "6                                원작의 긴장감을 제대로 살려내지못했다.\n",
       "7    별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...\n",
       "8                               액션이 없는데도 재미 있는 몇안되는 영화\n",
       "9        왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 문장 추출\n",
    "sentences = train['document']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]',\n",
       " '[CLS] 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 [SEP]',\n",
       " '[CLS] 너무재밓었다그래서보는것을추천한다 [SEP]',\n",
       " '[CLS] 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 [SEP]',\n",
       " '[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 [SEP]',\n",
       " '[CLS] 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. [SEP]',\n",
       " '[CLS] 원작의 긴장감을 제대로 살려내지못했다. [SEP]',\n",
       " '[CLS] 별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네 [SEP]',\n",
       " '[CLS] 액션이 없는데도 재미 있는 몇안되는 영화 [SEP]',\n",
       " '[CLS] 왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나? [SEP]']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 입력 형식에 맞게 변환\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![대체 텍스트](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n",
    "\n",
    "BERT의 입력은 위의 그림과 같은 형식입니다. Classification을 뜻하는 [CLS] 심볼이 제일 앞에 삽입됩니다. 파인튜닝시 출력에서 이 위치의 값을 사용하여 분류를 합니다. [SEP]은 Seperation을 가리키는데, 두 문장을 구분하는 역할을 합니다. 이 예제에서는 문장이 하나이므로 [SEP]도 하나만 넣습니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = train['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]\n",
      "['[CLS]', '아', '더', '##빙', '.', '.', '진', '##짜', '짜', '##증', '##나', '##네', '##요', '목', '##소', '##리', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT는 형태소분석으로 토큰을 분리하지 않습니다. WordPiece라는 통계적인 방식을 사용합니다. 한 단어내에서 자주 나오는 글자들을 붙여서 하나의 토큰으로 만듭니다. 이렇게 하면 언어에 상관없이 토큰을 생성할 수 있다는 장점이 있습니다. 또한 신조어 같이 사전에 없는 단어를 처리하기도 좋습니다.\n",
    "\n",
    "위의 결과에서 ## 기호는 앞 토큰과 이어진다는 표시입니다. 토크나이저는 여러 언어의 데이터를 기반으로 만든 'bert-base-multilingual-cased'를 사용합니다. 그래서 한글도 처리가 가능합니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519,   9074, 119005,    119,    119,   9708, 119235,\n",
       "         9715, 119230,  16439,  77884,  48549,   9284,  22333,  12692,\n",
       "          102,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 딥러닝 모델에는 토큰 자체를 입력으로 넣을 수 없습니다. 임베딩 레이어에는 토큰을 숫자로 된 인덱스로 변환하여 사용합니다. BERT의 토크나이저는 {단어토큰:인덱스}로 구성된 단어사전을 가지고 있습니다. 이를 참조하여 토큰을 인덱스로 바꿔줍니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9711,  11489,   9364,  41850,   9004,  32537,   9491,  35506,\n",
      "         17360,  48549,    119,    119,   9477,  26444,  12692,   9665,  21789,\n",
      "         11287,   9708, 119235,   9659,  22458, 119136,  12965,  48549,    119,\n",
      "           119,   9532,  22879,   9685,  16985,  14523,  48549,    119,    119,\n",
      "          9596, 118728,    119,    119,   9178, 106065, 118916,    119,    119,\n",
      "          8903,  11664,  11513,   9960,  14423,  25503, 118671,  48549,    119,\n",
      "           119,  21890,   9546,  37819,  22879,   9356,  14867,   9715, 119230,\n",
      "        118716,  48345,    119,   9663,  23321,  10954,   9638,  35506, 106320,\n",
      "         10739,  20173,   9359,  19105,  11102,  42428,  17196,  48549,    119,\n",
      "           119,    100,    117,   9947,  12945,   9532,  25503,   8932,  14423,\n",
      "         35506, 119050,  11903,  14867,  10003,  14863,  33188,  48345,    119,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(0)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "tensor([   101,   1871, 111754, 111754, 111754, 111754,    102,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 훈련셋과 검증셋으로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels,\n",
    "                                                                                    random_state=2018,\n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                    input_ids,\n",
    "                                                    random_state=2018,\n",
    "                                                    test_size=0.1)\n",
    "\n",
    "# 데이터를 파이토치의 텐서로 변환\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 32\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **전처리 - 테스트셋**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  굳 ㅋ\n",
       "1                                 GDNTOPCLASSINTHECLUB\n",
       "2               뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
       "3                     지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n",
       "4    3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n",
       "5                                   음악이 주가 된, 최고의 음악영화\n",
       "6                                              진정한 쓰레기\n",
       "7             마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
       "8    갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한c...\n",
       "9       이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 문장 추출\n",
    "sentences = test['document']\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 굳 ㅋ [SEP]',\n",
       " '[CLS] GDNTOPCLASSINTHECLUB [SEP]',\n",
       " '[CLS] 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아 [SEP]',\n",
       " '[CLS] 지루하지는 않은데 완전 막장임... 돈주고 보기에는.... [SEP]',\n",
       " '[CLS] 3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠?? [SEP]',\n",
       " '[CLS] 음악이 주가 된, 최고의 음악영화 [SEP]',\n",
       " '[CLS] 진정한 쓰레기 [SEP]',\n",
       " '[CLS] 마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다 [SEP]',\n",
       " '[CLS] 갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다 [SEP]',\n",
       " '[CLS] 이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네.. [SEP]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT의 입력 형식에 맞게 변환\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨 추출\n",
    "labels = test['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 굳 ㅋ [SEP]\n",
      "['[CLS]', '굳', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 파이토치의 텐서로 변환\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 32\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **모델 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)  # GPU 디바이스 이름 \n",
    "    print(f'Found GPU: {device_name}')\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류를 위한 BERT 모델 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![대체 텍스트](http://www.mccormickml.com/assets/BERT/padding_and_mask.png)\n",
    "\n",
    "사전훈련된 BERT는 다양한 문제로 전이학습이 가능합니다. 여기서는 위의 그림과 같이 한 문장을 분류하는 방법을 사용합니다. 영화리뷰 문장이 입력으로 들어가면, 긍정/부정으로 구분합니다. 모델의 출력에서 [CLS] 위치인 첫 번째 토큰에 새로운 레이어를 붙여서 파인튜닝을 합니다. Huggning Face는 BertForSequenceClassification() 함수를 제공하기 때문에 쉽게 구현할 수 있습니다.\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4\n",
    "\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:08:08.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 0:16:10.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 0:24:39.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 0:32:41.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 1:02:12.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 1:32:09.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 2:03:41.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 2:34:19.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 2:48:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:05:22\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:33:58.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 1:06:44.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 1:38:41.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 1:56:58.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 2:04:58.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 2:12:56.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 2:20:54.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 2:30:05.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 2:33:35\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:07:59.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 0:15:57.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 0:23:56.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 0:32:06.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 0:40:07.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 0:48:16.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 0:56:15.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 1:04:14.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epcoh took: 1:07:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:01:35\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch   500  of  4,219.    Elapsed: 0:07:58.\n",
      "  Batch 1,000  of  4,219.    Elapsed: 0:15:58.\n",
      "  Batch 1,500  of  4,219.    Elapsed: 0:23:57.\n",
      "  Batch 2,000  of  4,219.    Elapsed: 0:41:06.\n",
      "  Batch 2,500  of  4,219.    Elapsed: 0:49:36.\n",
      "  Batch 3,000  of  4,219.    Elapsed: 0:58:07.\n",
      "  Batch 3,500  of  4,219.    Elapsed: 1:06:37.\n",
      "  Batch 4,000  of  4,219.    Elapsed: 1:15:06.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 1:18:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:01:39\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():\n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        # 출력 로짓 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭마다 훈련셋과 검증셋을 반복하여 학습을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **테스트셋 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,563.    Elapsed: 0:00:21.\n",
      "  Batch   200  of  1,563.    Elapsed: 0:00:42.\n",
      "  Batch   300  of  1,563.    Elapsed: 0:01:03.\n",
      "  Batch   400  of  1,563.    Elapsed: 0:01:25.\n",
      "  Batch   500  of  1,563.    Elapsed: 0:01:46.\n",
      "  Batch   600  of  1,563.    Elapsed: 0:02:07.\n",
      "  Batch   700  of  1,563.    Elapsed: 0:02:28.\n",
      "  Batch   800  of  1,563.    Elapsed: 0:02:49.\n",
      "  Batch   900  of  1,563.    Elapsed: 0:03:10.\n",
      "  Batch 1,000  of  1,563.    Elapsed: 0:03:32.\n",
      "  Batch 1,100  of  1,563.    Elapsed: 0:03:53.\n",
      "  Batch 1,200  of  1,563.    Elapsed: 0:04:14.\n",
      "  Batch 1,300  of  1,563.    Elapsed: 0:04:35.\n",
      "  Batch 1,400  of  1,563.    Elapsed: 0:04:57.\n",
      "  Batch 1,500  of  1,563.    Elapsed: 0:05:18.\n",
      "\n",
      "Accuracy: 0.87\n",
      "Test took: 0:05:31\n"
     ]
    }
   ],
   "source": [
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():\n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 출력 로짓 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트셋의 정확도가 87%입니다. <BERT 톺아보기> 블로그에서는 같은 데이터로 88.7%를 달성하였습니다. 거기서는 한글 코퍼스로 사전훈련을 하여 새로운 모델을 만들었습니다. 반면에 우리는 BERT의 기본 모델인 bert-base-multilingual-cased를 사용했기 때문에 더 성능이 낮은 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **새로운 문장 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "\n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():\n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 출력 로짓 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.2769246  1.905877 ]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['연기는 별로지만 재미 하나는 끝내줌!'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3848722 -3.0497596]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['주연배우가 아깝다. 총체적 난국...'])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습한 모델을 가지고 실제 문장을 넣어봤습니다.\n",
    "\n",
    "출력 로짓은 소프트맥스가 적용되지 않은 상태입니다.\n",
    "\n",
    " argmax로 더 높은 값의 위치를 라벨로 설정하면 됩니다. 0은 부정, 1은 긍정입니다. \n",
    " \n",
    " 위와 같이 새로운 문장에도 잘 분류를 하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 챗봇 개발자 모임 ><br>\n",
    "- 페이스북 그룹에 가입하시면 챗봇에 대한 최신 정보를 쉽게 받으실 수 있습니다.\n",
    "- https://www.facebook.com/groups/ChatbotDevKR/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
